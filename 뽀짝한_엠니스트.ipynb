{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "뽀짝한 엠니스트",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAgyNFncSqfx"
      },
      "source": [
        "# < 첫 모델 : SGD 경사 하강법>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b11_9K-MSsj5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os as os\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA1bAsWRSspS"
      },
      "source": [
        "#train 데이터 불러오기\r\n",
        "from google.colab import files\r\n",
        "file_uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKaWSjOMSsuc"
      },
      "source": [
        "#test 데이터 불러오기\r\n",
        "from google.colab import files\r\n",
        "file_uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfLlBny-SsyT"
      },
      "source": [
        "train_orig = pd.read_csv('train.csv')\r\n",
        "test_orig = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUhTwqISs5G"
      },
      "source": [
        "X = train_orig.drop(['label'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7aG9gKvSs-1"
      },
      "source": [
        "label = train_orig.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B5jA2NpStDB"
      },
      "source": [
        "y = pd.get_dummies(label, columns=['label'], drop_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC5P7-khStG-"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 1004)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulGGyq5KStKz"
      },
      "source": [
        "print(\"Shape of x_train :\", x_train.shape)\r\n",
        "print(\"Shape of x_test :\", x_test.shape)\r\n",
        "print(\"Shape of y_train :\", y_train.shape)\r\n",
        "print(\"Shape of y_test :\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyYcbhhmTNYF"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\r\n",
        "mnist = fetch_openml('mnist_784', version =1)\r\n",
        "mnist.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJmHMcl9TNeE"
      },
      "source": [
        "X,y = mnist[\"data\"], mnist[\"target\"]\r\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-n79W1UTNln"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqAXgwaaStOy"
      },
      "source": [
        "mport matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "some_digit = X[0]\r\n",
        "some_digit_image = some_digit.reshape(28,28)\r\n",
        "\r\n",
        "#이미지의 feature 벡터를 28*28 배열로 크기 변환\r\n",
        "\r\n",
        "plt.imshow(some_digit_image, cmap = \"binary\")\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show\r\n",
        "\r\n",
        "#imshow 함수를 사용해 그려주기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q17ePL5HTXDz"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5ukcyxHTctv"
      },
      "source": [
        "# 결과값이 문자열로 나옴.\r\n",
        "# 머신러닝 알고리즘에서는 숫자를 사용하여야 하므로 y를 정수로 변환해줘야함"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEa5cVSSTXIK"
      },
      "source": [
        "import numpy as np\r\n",
        "y = y.astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVFtBxpkTXO9"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNwmhuSTXo3"
      },
      "source": [
        "def plot_digits(instances, images_per_row=10, **options):\r\n",
        "    size = 28\r\n",
        "    images_per_row = min(len(instances), images_per_row)\r\n",
        "    images = [instance.reshape(size,size) for instance in instances]\r\n",
        "    n_rows = (len(instances) - 1) // images_per_row + 1\r\n",
        "    row_images = []\r\n",
        "    n_empty = n_rows * images_per_row - len(instances)\r\n",
        "    images.append(np.zeros((size,size * n_empty)))\r\n",
        "    for row in range(n_rows):\r\n",
        "        rimages = images[row * images_per_row : (row +1) * images_per_row]\r\n",
        "        row_images.append(np.concatenate(rimages,axis=1))\r\n",
        "    image = np.concatenate(row_images,axis=0)\r\n",
        "    plt.imshow(image,cmap = mpl.cm.binary, **options)\r\n",
        "    plt.axis(\"off\")\r\n",
        "    \r\n",
        "plt.figure(figsize=(9,9))\r\n",
        "example_images = X[:100]\r\n",
        "plot_digits(example_images, images_per_row=10)\r\n",
        "#save_fig(\"more_digits_plot\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxLv7UwATqWy"
      },
      "source": [
        "# 데이터 분리\r\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000],y[60000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dfi0J9WTqbL"
      },
      "source": [
        "y_train_5 = (y_train == 5)\r\n",
        "y_test_5 = (y_test == 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Oqi2OTTqlm"
      },
      "source": [
        "# 첫번째 모델 : SGDClassifier클래스의 확률적 경사 하강법\r\n",
        "# SGD는 Loss function 계산 시 전체가 아닌 일부 데이터셋을 이용하기 때문에 속도가 빨라서 매우 큰 데이터셋을 다르는데 효과적"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPwDCB5ETqrV"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\r\n",
        "\r\n",
        "sgd_clf = SGDClassifier(random_state = 42)\r\n",
        "sgd_clf.fit(X_train, y_train_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9OVZJV2T22g"
      },
      "source": [
        "sgd_clf.predict([some_digit])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnZVtDfST27s"
      },
      "source": [
        "# 성능측정\r\n",
        "# 1.cross validation을 사용한 accuracy측정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csLTUXk5T7gT"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.base import clone\r\n",
        "skfolds = StratifiedKFold(n_splits=3,random_state = 42)\r\n",
        "\r\n",
        "for train_index, test_index in skfolds.split(X_train, y_train_5):\r\n",
        "    clone_clf = clone(sgd_clf)\r\n",
        "    X_train_folds = X_train[train_index]\r\n",
        "    y_train_folds = y_train_5[train_index]\r\n",
        "    X_test_fold = X_train[test_index]\r\n",
        "    y_test_fold = y_train_5[test_index]\r\n",
        "    \r\n",
        "    clone_clf.fit(X_train_folds, y_train_folds)\r\n",
        "    y_pred = clone_clf.predict(X_test_fold)\r\n",
        "    n_correct = sum(y_pred == y_test_fold)\r\n",
        "    print(n_correct/len(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Snd-dkBT7mZ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "cross_val_score(sgd_clf, X_train, y_train_5, cv = 3, scoring = \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzdMBVuUT7st"
      },
      "source": [
        "from sklearn.base import BaseEstimator\r\n",
        "\r\n",
        "class Never5Classifier(BaseEstimator):\r\n",
        "    def fit(self,X,y=None):\r\n",
        "        return self\r\n",
        "    def predict(self,X):\r\n",
        "        return np.zeros((len(X),1),dtype=bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqtxyV0FT7yk"
      },
      "source": [
        "never_5_clf = Never5Classifier()\r\n",
        "cross_val_score(never_5_clf,X_train,y_train_5,cv=3,scoring=\"accuracy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxyEiS4fT73V"
      },
      "source": [
        "# 성능측정\r\n",
        "# 2.오차행렬"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQX1qLAmTX5D"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\r\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPLmkpcAUIx7"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "confusion_matrix(y_train_5,y_train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnjfMKW4UI2I"
      },
      "source": [
        "# 3.5 에러 분석\r\n",
        "# 오차 행렬 살펴보기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpCzVlO1UI7f"
      },
      "source": [
        "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\r\n",
        "conf_mx = confusion_matrix(y_train, y_train_pred)\r\n",
        "conf_mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSEbOxdwUJCc"
      },
      "source": [
        "# 예측 틀린 데이터 시각화\r\n",
        "plt.matshow(conf_mx, cmap = plt.cm.gray)\r\n",
        "plt.show ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXoAc9XDUJLF"
      },
      "source": [
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\r\n",
        "norm_conf_mx = conf_mx / row_sums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EGFdtuMUJUj"
      },
      "source": [
        "np.fill_diagonal(norm_conf_mx, 0)\r\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnMUo1A9UaeJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_YxmLpz3iiH"
      },
      "source": [
        "# < 케라스로 CNN 튜토리얼 해보기 >"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LthaFVjN546Q",
        "outputId": "65f16806-af0c-4985-c0e9-5e62a77b7ff8"
      },
      "source": [
        "#필요한 모듈 불러오기\r\n",
        "import warnings\r\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
        "\r\n",
        "import sys\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Flatten\r\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\r\n",
        "import numpy as np\r\n",
        "np.random.seed(7)\r\n",
        "\r\n",
        "print('Python version : ', sys.version)\r\n",
        "print('TensorFlow version : ', tf.__version__)\r\n",
        "print('Keras version : ', keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version :  3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "TensorFlow version :  2.4.1\n",
            "Keras version :  2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpD_sBoJ5837",
        "outputId": "dbf39efa-6104-4f15-8061-cdfc9b8d85d0"
      },
      "source": [
        "# keras에서 제공하는 MNIST데이터를 받고, 사용할 수 있게 train과 test 데이터를 만들어줌\r\n",
        "img_rows = 28\r\n",
        "img_cols = 28\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
        "\r\n",
        "input_shape = (img_rows, img_cols, 1)\r\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
        "\r\n",
        "x_train = x_train.astype('float32') / 255.\r\n",
        "x_test = x_test.astype('float32') / 255.\r\n",
        "\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print(x_train.shape[0], 'train samples')\r\n",
        "print(x_test.shape[0], 'test samples')\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "num_classes = 10\r\n",
        "epochs = 12\r\n",
        "\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upz0ht6T6BhU",
        "outputId": "512f25b6-7865-44c4-9d76-0551dab1f27b"
      },
      "source": [
        "#입출력 관계와 층구조\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), padding='same',\r\n",
        "                 activation='relu',\r\n",
        "                 input_shape=input_shape))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n",
        "model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(1000, activation='relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1000)              3137000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10010     \n",
            "=================================================================\n",
            "Total params: 3,156,098\n",
            "Trainable params: 3,156,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "tjXnAN9n6Hj-",
        "outputId": "872f02a3-54c3-4b1b-8144-a43b16cf3559"
      },
      "source": [
        "#시각화\r\n",
        "from IPython.display import SVG\r\n",
        "from keras.utils.vis_utils import model_to_dot\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "SVG(model_to_dot(model, show_shapes=True,dpi=55).create(prog='dot', format='svg'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"613pt\" viewBox=\"0.00 0.00 412.00 802.00\" width=\"315pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.7639 .7639) rotate(0) translate(4 798)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-798 408,-798 408,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140048777225608 -->\n<g class=\"node\" id=\"node1\">\n<title>140048777225608</title>\n<polygon fill=\"none\" points=\"25.5,-747.5 25.5,-793.5 378.5,-793.5 378.5,-747.5 25.5,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110\" y=\"-766.8\">conv2d_input: InputLayer</text>\n<polyline fill=\"none\" points=\"194.5,-747.5 194.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"194.5,-770.5 252.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"252.5,-747.5 252.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315.5\" y=\"-778.3\">[(None, 28, 28, 1)]</text>\n<polyline fill=\"none\" points=\"252.5,-770.5 378.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315.5\" y=\"-755.3\">[(None, 28, 28, 1)]</text>\n</g>\n<!-- 140048789044808 -->\n<g class=\"node\" id=\"node2\">\n<title>140048789044808</title>\n<polygon fill=\"none\" points=\"51.5,-664.5 51.5,-710.5 352.5,-710.5 352.5,-664.5 51.5,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.5\" y=\"-683.8\">conv2d: Conv2D</text>\n<polyline fill=\"none\" points=\"169.5,-664.5 169.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"169.5,-687.5 227.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"227.5,-664.5 227.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-695.3\">(None, 28, 28, 1)</text>\n<polyline fill=\"none\" points=\"227.5,-687.5 352.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-672.3\">(None, 28, 28, 32)</text>\n</g>\n<!-- 140048777225608&#45;&gt;140048789044808 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140048777225608-&gt;140048789044808</title>\n<path d=\"M202,-747.3799C202,-739.1745 202,-729.7679 202,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-720.784 202,-710.784 198.5001,-720.784 205.5001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048711086880 -->\n<g class=\"node\" id=\"node3\">\n<title>140048711086880</title>\n<polygon fill=\"none\" points=\"7.5,-581.5 7.5,-627.5 396.5,-627.5 396.5,-581.5 7.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.5\" y=\"-600.8\">max_pooling2d: MaxPooling2D</text>\n<polyline fill=\"none\" points=\"213.5,-581.5 213.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"213.5,-604.5 271.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"271.5,-581.5 271.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-612.3\">(None, 28, 28, 32)</text>\n<polyline fill=\"none\" points=\"271.5,-604.5 396.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-589.3\">(None, 14, 14, 32)</text>\n</g>\n<!-- 140048789044808&#45;&gt;140048711086880 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140048789044808-&gt;140048711086880</title>\n<path d=\"M202,-664.3799C202,-656.1745 202,-646.7679 202,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-637.784 202,-627.784 198.5001,-637.784 205.5001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710742984 -->\n<g class=\"node\" id=\"node4\">\n<title>140048710742984</title>\n<polygon fill=\"none\" points=\"44,-498.5 44,-544.5 360,-544.5 360,-498.5 44,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.5\" y=\"-517.8\">conv2d_1: Conv2D</text>\n<polyline fill=\"none\" points=\"177,-498.5 177,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"177,-521.5 235,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"235,-498.5 235,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297.5\" y=\"-529.3\">(None, 14, 14, 32)</text>\n<polyline fill=\"none\" points=\"235,-521.5 360,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297.5\" y=\"-506.3\">(None, 14, 14, 64)</text>\n</g>\n<!-- 140048711086880&#45;&gt;140048710742984 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140048711086880-&gt;140048710742984</title>\n<path d=\"M202,-581.3799C202,-573.1745 202,-563.7679 202,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-554.784 202,-544.784 198.5001,-554.784 205.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710745952 -->\n<g class=\"node\" id=\"node5\">\n<title>140048710745952</title>\n<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 404,-461.5 404,-415.5 0,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.5\" y=\"-434.8\">max_pooling2d_1: MaxPooling2D</text>\n<polyline fill=\"none\" points=\"221,-415.5 221,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"221,-438.5 279,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"279,-415.5 279,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"341.5\" y=\"-446.3\">(None, 14, 14, 64)</text>\n<polyline fill=\"none\" points=\"279,-438.5 404,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"341.5\" y=\"-423.3\">(None, 7, 7, 64)</text>\n</g>\n<!-- 140048710742984&#45;&gt;140048710745952 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140048710742984-&gt;140048710745952</title>\n<path d=\"M202,-498.3799C202,-490.1745 202,-480.7679 202,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-471.784 202,-461.784 198.5001,-471.784 205.5001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710746064 -->\n<g class=\"node\" id=\"node6\">\n<title>140048710746064</title>\n<polygon fill=\"none\" points=\"58.5,-332.5 58.5,-378.5 345.5,-378.5 345.5,-332.5 58.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118\" y=\"-351.8\">dropout: Dropout</text>\n<polyline fill=\"none\" points=\"177.5,-332.5 177.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"177.5,-355.5 235.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"235.5,-332.5 235.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-363.3\">(None, 7, 7, 64)</text>\n<polyline fill=\"none\" points=\"235.5,-355.5 345.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-340.3\">(None, 7, 7, 64)</text>\n</g>\n<!-- 140048710745952&#45;&gt;140048710746064 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140048710745952-&gt;140048710746064</title>\n<path d=\"M202,-415.3799C202,-407.1745 202,-397.7679 202,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-388.784 202,-378.784 198.5001,-388.784 205.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710774344 -->\n<g class=\"node\" id=\"node7\">\n<title>140048710774344</title>\n<polygon fill=\"none\" points=\"69,-249.5 69,-295.5 335,-295.5 335,-249.5 69,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118\" y=\"-268.8\">flatten: Flatten</text>\n<polyline fill=\"none\" points=\"167,-249.5 167,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"167,-272.5 225,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"225,-249.5 225,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280\" y=\"-280.3\">(None, 7, 7, 64)</text>\n<polyline fill=\"none\" points=\"225,-272.5 335,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280\" y=\"-257.3\">(None, 3136)</text>\n</g>\n<!-- 140048710746064&#45;&gt;140048710774344 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140048710746064-&gt;140048710774344</title>\n<path d=\"M202,-332.3799C202,-324.1745 202,-314.7679 202,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-305.784 202,-295.784 198.5001,-305.784 205.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710797128 -->\n<g class=\"node\" id=\"node8\">\n<title>140048710797128</title>\n<polygon fill=\"none\" points=\"79.5,-166.5 79.5,-212.5 324.5,-212.5 324.5,-166.5 79.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125.5\" y=\"-185.8\">dense: Dense</text>\n<polyline fill=\"none\" points=\"171.5,-166.5 171.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"171.5,-189.5 229.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"229.5,-166.5 229.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"277\" y=\"-197.3\">(None, 3136)</text>\n<polyline fill=\"none\" points=\"229.5,-189.5 324.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"277\" y=\"-174.3\">(None, 1000)</text>\n</g>\n<!-- 140048710774344&#45;&gt;140048710797128 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140048710774344-&gt;140048710797128</title>\n<path d=\"M202,-249.3799C202,-241.1745 202,-231.7679 202,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-222.784 202,-212.784 198.5001,-222.784 205.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710797520 -->\n<g class=\"node\" id=\"node9\">\n<title>140048710797520</title>\n<polygon fill=\"none\" points=\"58.5,-83.5 58.5,-129.5 345.5,-129.5 345.5,-83.5 58.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125.5\" y=\"-102.8\">dropout_1: Dropout</text>\n<polyline fill=\"none\" points=\"192.5,-83.5 192.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"192.5,-106.5 250.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"250.5,-83.5 250.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-114.3\">(None, 1000)</text>\n<polyline fill=\"none\" points=\"250.5,-106.5 345.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-91.3\">(None, 1000)</text>\n</g>\n<!-- 140048710797128&#45;&gt;140048710797520 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140048710797128-&gt;140048710797520</title>\n<path d=\"M202,-166.3799C202,-158.1745 202,-148.7679 202,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-139.784 202,-129.784 198.5001,-139.784 205.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140048710885952 -->\n<g class=\"node\" id=\"node10\">\n<title>140048710885952</title>\n<polygon fill=\"none\" points=\"72,-.5 72,-46.5 332,-46.5 332,-.5 72,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125.5\" y=\"-19.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"179,-.5 179,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"208\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"179,-23.5 237,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"208\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"237,-.5 237,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"284.5\" y=\"-31.3\">(None, 1000)</text>\n<polyline fill=\"none\" points=\"237,-23.5 332,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"284.5\" y=\"-8.3\">(None, 10)</text>\n</g>\n<!-- 140048710797520&#45;&gt;140048710885952 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140048710797520-&gt;140048710885952</title>\n<path d=\"M202,-83.3799C202,-75.1745 202,-65.7679 202,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-56.784 202,-46.784 198.5001,-56.784 205.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ay2v2p_6La4",
        "outputId": "0bdcbdcb-1be2-4954-fb7e-d1181df3dcc3"
      },
      "source": [
        "#fit~ 시작\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "hist = model.fit(x_train, y_train,\r\n",
        "                 batch_size=batch_size,\r\n",
        "                 epochs=epochs,\r\n",
        "                 verbose=1, \r\n",
        "                 validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "469/469 [==============================] - 82s 172ms/step - loss: 0.4373 - accuracy: 0.8615 - val_loss: 0.0413 - val_accuracy: 0.9866\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 82s 176ms/step - loss: 0.0618 - accuracy: 0.9802 - val_loss: 0.0386 - val_accuracy: 0.9863\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 81s 173ms/step - loss: 0.0456 - accuracy: 0.9854 - val_loss: 0.0296 - val_accuracy: 0.9905\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 81s 173ms/step - loss: 0.0370 - accuracy: 0.9881 - val_loss: 0.0281 - val_accuracy: 0.9914\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 85s 182ms/step - loss: 0.0291 - accuracy: 0.9911 - val_loss: 0.0231 - val_accuracy: 0.9932\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 81s 172ms/step - loss: 0.0254 - accuracy: 0.9927 - val_loss: 0.0198 - val_accuracy: 0.9938\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 80s 171ms/step - loss: 0.0226 - accuracy: 0.9920 - val_loss: 0.0249 - val_accuracy: 0.9931\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 80s 171ms/step - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.0282 - val_accuracy: 0.9923\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 80s 170ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.0247 - val_accuracy: 0.9929\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 80s 171ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0213 - val_accuracy: 0.9937\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 81s 172ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.0231 - val_accuracy: 0.9928\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 84s 180ms/step - loss: 0.0123 - accuracy: 0.9953 - val_loss: 0.0273 - val_accuracy: 0.9925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiHr1s_08raQ",
        "outputId": "92796223-beea-4104-f7d8-25424cd3d176"
      },
      "source": [
        "#test 데이터에 대한 accuracy\r\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.025037094950675964\n",
            "Test accuracy: 0.9923999905586243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "OOvMK_rO8rqA",
        "outputId": "8c3e708c-e886-41f8-d61e-07a0a780a341"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "n = 0\r\n",
        "plt.imshow(x_test[n].reshape(28, 28), cmap='Greys', interpolation='nearest')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('The Answer is ', model.predict_classes(x_test[n].reshape((1, 28, 28, 1))))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The Answer is  [7]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "6ghOS9ht8rwh",
        "outputId": "ed970222-f8d2-4fcb-aa7e-6e5e045fa7da"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "predicted_result = model.predict(x_test)\r\n",
        "predicted_labels = np.argmax(predicted_result, axis=1)\r\n",
        "\r\n",
        "test_labels = np.argmax(y_test, axis=1)\r\n",
        "\r\n",
        "wrong_result = []\r\n",
        "\r\n",
        "for n in range(0, len(test_labels)):\r\n",
        "    if predicted_labels[n] != test_labels[n]:\r\n",
        "        wrong_result.append(n)\r\n",
        "\r\n",
        "samples = random.choices(population=wrong_result, k=16)\r\n",
        "\r\n",
        "count = 0\r\n",
        "nrows = ncols = 4\r\n",
        "\r\n",
        "plt.figure(figsize=(12,8))\r\n",
        "\r\n",
        "for n in samples:\r\n",
        "    count += 1\r\n",
        "    plt.subplot(nrows, ncols, count)\r\n",
        "    plt.imshow(x_test[n].reshape(28, 28), cmap='Greys', interpolation='nearest')\r\n",
        "    tmp = \"Label:\" + str(test_labels[n]) + \", Prediction:\" + str(predicted_labels[n])\r\n",
        "    plt.title(tmp)\r\n",
        "\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97ydhVJY8pmE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNMNmFQhD3xe"
      },
      "source": [
        "# < 텐서플로우 2.0 버전 >"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIh7MLHuYN0t",
        "outputId": "f37b4336-b554-46b0-b5be-5cdca23b495c"
      },
      "source": [
        "#라이브러리 임포트하기\r\n",
        "!pip install -q tensorflow-gpu==2.0.0-rc1\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\r\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 380.5MB 43kB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 27.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3MB 14.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL3sIkQOYOnh",
        "outputId": "27aa2b41-822d-4b7e-ad9f-548fa62cdc1a"
      },
      "source": [
        "#mnist 데이터 로드\r\n",
        "mnist = tf.keras.datasets.mnist\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
        "\r\n",
        "# 채널 차원을 추가합니다.\r\n",
        "x_train = x_train[..., tf.newaxis]\r\n",
        "x_test = x_test[..., tf.newaxis]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYlywTwMYhvR"
      },
      "source": [
        "#tf.data를 사용하여 데이터셋을 섞고 배치를 만들기\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (x_train, y_train)).shuffle(10000).batch(32)\r\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-h-XNAPYpc8"
      },
      "source": [
        "#케라스의 모델 서브클래싱(subclassing) API를 사용하여 tf.keras 모델을 만듬\r\n",
        "class MyModel(Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(MyModel, self).__init__()\r\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu')\r\n",
        "    self.flatten = Flatten()\r\n",
        "    self.d1 = Dense(128, activation='relu')\r\n",
        "    self.d2 = Dense(10, activation='softmax')\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.d1(x)\r\n",
        "    return self.d2(x)\r\n",
        "\r\n",
        "model = MyModel()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OK24SZuY0QK"
      },
      "source": [
        "#훈련에 필요한 옵티마이저(optimizer)와 손실 함수를 선택\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI6iHEqXY6H2"
      },
      "source": [
        "#모델의 손실과 성능을 측정할 지표를 선택, 에포크가 진행되는 동안 수집된 측정 지표를 바탕으로 최종 결과를 출력\r\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n",
        "\r\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCV8SjKsZUln"
      },
      "source": [
        "#tf.GradientTape를 사용하여 모델을 훈련\r\n",
        "@tf.function\r\n",
        "def train_step(images, labels):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    predictions = model(images)\r\n",
        "    loss = loss_object(labels, predictions)\r\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "  train_loss(loss)\r\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQYxbmfcZfD1"
      },
      "source": [
        "#모델 테스트\r\n",
        "@tf.function\r\n",
        "def test_step(images, labels):\r\n",
        "  predictions = model(images)\r\n",
        "  t_loss = loss_object(labels, predictions)\r\n",
        "\r\n",
        "  test_loss(t_loss)\r\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iUWPYgwZkjv",
        "outputId": "09fa5b05-6e34-40f1-8f24-5d0d1b62760f"
      },
      "source": [
        "EPOCHS = 6\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  for images, labels in train_ds:\r\n",
        "    train_step(images, labels)\r\n",
        "\r\n",
        "  for test_images, test_labels in test_ds:\r\n",
        "    test_step(test_images, test_labels)\r\n",
        "\r\n",
        "  template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\r\n",
        "  print (template.format(epoch+1,\r\n",
        "                         train_loss.result(),\r\n",
        "                         train_accuracy.result()*100,\r\n",
        "                         test_loss.result(),\r\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function test_step at 0x7f2b25ecf488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING: Entity <function test_step at 0x7f2b25ecf488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f2b2c5dee48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f2b2c5dee48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f2b2c5dee48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f2b2c5dee48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "에포크: 1, 손실: 0.10776229202747345, 정확도: 96.72856903076172, 테스트 손실: 0.05782943218946457, 테스트 정확도: 98.11000061035156\n",
            "에포크: 2, 손실: 0.0757531151175499, 정확도: 97.68402862548828, 테스트 손실: 0.05104268714785576, 테스트 정확도: 98.33999633789062\n",
            "에포크: 3, 손실: 0.05839025229215622, 정확도: 98.20735168457031, 테스트 손실: 0.05299009010195732, 테스트 정확도: 98.27333068847656\n",
            "에포크: 4, 손실: 0.0477835051715374, 정확도: 98.52424621582031, 테스트 손실: 0.05637459084391594, 테스트 정확도: 98.22999572753906\n",
            "에포크: 5, 손실: 0.04045642912387848, 정확도: 98.75061798095703, 테스트 손실: 0.059136953204870224, 테스트 정확도: 98.23400115966797\n",
            "에포크: 6, 손실: 0.03517358750104904, 정확도: 98.90963745117188, 테스트 손실: 0.06164858490228653, 테스트 정확도: 98.24500274658203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFqX6c9hVV3k"
      },
      "source": [
        "# 2,3 에서 조금 차이 , 그 뒤로는 차이가 별로 없음 -> 3 정도면 충분할 듯"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Qr0MGah5PA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-e5RLDqeZfD"
      },
      "source": [
        "# < MNIST를 다층 퍼셉트론 모델로 학습 MLP >"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aahl773HYq3g"
      },
      "source": [
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "from keras.layers import Dense, Activation\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "np.random.seed(3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x4aX4LV2Yr2S",
        "outputId": "7088461e-df07-48f4-af38-caa01756fb34"
      },
      "source": [
        "# 1. 데이터셋 준비하기\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 훈련셋과 시험셋 로딩\r\n",
        "\r\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 훈련셋과 검증셋 분리\r\n",
        "\r\n",
        "X_val = X_train[50000:]\r\n",
        "\r\n",
        "Y_val = Y_train[50000:]\r\n",
        "\r\n",
        "X_train = X_train[:50000]\r\n",
        "\r\n",
        "Y_train = Y_train[:50000]\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\r\n",
        "\r\n",
        "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\r\n",
        "\r\n",
        "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 훈련셋, 검증셋 고르기\r\n",
        "\r\n",
        "train_rand_idxs = np.random.choice(50000, 700)\r\n",
        "\r\n",
        "val_rand_idxs = np.random.choice(10000, 300)\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "X_train = X_train[train_rand_idxs]\r\n",
        "\r\n",
        "Y_train = Y_train[train_rand_idxs]\r\n",
        "\r\n",
        "X_val = X_val[val_rand_idxs]\r\n",
        "\r\n",
        "Y_val = Y_val[val_rand_idxs]\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 라벨링 전환\r\n",
        "\r\n",
        "Y_train = np_utils.to_categorical(Y_train)\r\n",
        "\r\n",
        "Y_val = np_utils.to_categorical(Y_val)\r\n",
        "\r\n",
        "Y_test = np_utils.to_categorical(Y_test)\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 2. 모델 구성하기\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 3. 모델 엮기\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 4. 모델 학습시키기\r\n",
        "\r\n",
        "hist = model.fit(X_train, Y_train, epochs=1000, batch_size=10, validation_data=(X_val, Y_val))\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# 5. 모델 학습 과정 표시하기\r\n",
        "\r\n",
        "# %matplotlib inline\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "fig, loss_ax = plt.subplots()\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "acc_ax = loss_ax.twinx()\r\n",
        "\r\n",
        " \r\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\r\n",
        "\r\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\r\n",
        "\r\n",
        "\r\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\r\n",
        "\r\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\r\n",
        "\r\n",
        " \r\n",
        "loss_ax.set_xlabel('epoch')\r\n",
        "\r\n",
        "loss_ax.set_ylabel('loss')\r\n",
        "\r\n",
        "acc_ax.set_ylabel('accuray')\r\n",
        "\r\n",
        "loss_ax.legend(loc='upper left')\r\n",
        "\r\n",
        "acc_ax.legend(loc='lower left')\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "70/70 [==============================] - 1s 4ms/step - loss: 2.3057 - accuracy: 0.1188 - val_loss: 2.2809 - val_accuracy: 0.1500\n",
            "Epoch 2/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 2.2687 - accuracy: 0.1407 - val_loss: 2.2344 - val_accuracy: 0.1600\n",
            "Epoch 3/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 2.2319 - accuracy: 0.1311 - val_loss: 2.1664 - val_accuracy: 0.2133\n",
            "Epoch 4/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 2.1479 - accuracy: 0.2378 - val_loss: 2.1009 - val_accuracy: 0.2567\n",
            "Epoch 5/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 2.0777 - accuracy: 0.2418 - val_loss: 2.0519 - val_accuracy: 0.2733\n",
            "Epoch 6/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 2.0310 - accuracy: 0.2506 - val_loss: 2.0117 - val_accuracy: 0.2633\n",
            "Epoch 7/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.9879 - accuracy: 0.2542 - val_loss: 1.9756 - val_accuracy: 0.2633\n",
            "Epoch 8/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.9444 - accuracy: 0.3048 - val_loss: 1.9446 - val_accuracy: 0.2767\n",
            "Epoch 9/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.9095 - accuracy: 0.3078 - val_loss: 1.9158 - val_accuracy: 0.2900\n",
            "Epoch 10/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.8737 - accuracy: 0.2982 - val_loss: 1.8925 - val_accuracy: 0.2800\n",
            "Epoch 11/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.8344 - accuracy: 0.3024 - val_loss: 1.8657 - val_accuracy: 0.3000\n",
            "Epoch 12/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.8088 - accuracy: 0.3302 - val_loss: 1.8441 - val_accuracy: 0.3067\n",
            "Epoch 13/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.7403 - accuracy: 0.3544 - val_loss: 1.8235 - val_accuracy: 0.3133\n",
            "Epoch 14/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.7729 - accuracy: 0.3353 - val_loss: 1.8081 - val_accuracy: 0.3100\n",
            "Epoch 15/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.7079 - accuracy: 0.3931 - val_loss: 1.7875 - val_accuracy: 0.3467\n",
            "Epoch 16/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6971 - accuracy: 0.3639 - val_loss: 1.7685 - val_accuracy: 0.3300\n",
            "Epoch 17/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6548 - accuracy: 0.4020 - val_loss: 1.7511 - val_accuracy: 0.3533\n",
            "Epoch 18/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6897 - accuracy: 0.4079 - val_loss: 1.7354 - val_accuracy: 0.3567\n",
            "Epoch 19/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6433 - accuracy: 0.4467 - val_loss: 1.7225 - val_accuracy: 0.3733\n",
            "Epoch 20/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6625 - accuracy: 0.4424 - val_loss: 1.7063 - val_accuracy: 0.3867\n",
            "Epoch 21/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6297 - accuracy: 0.4436 - val_loss: 1.6916 - val_accuracy: 0.4100\n",
            "Epoch 22/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.6171 - accuracy: 0.4808 - val_loss: 1.6800 - val_accuracy: 0.4100\n",
            "Epoch 23/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.5554 - accuracy: 0.4953 - val_loss: 1.6722 - val_accuracy: 0.4033\n",
            "Epoch 24/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.5731 - accuracy: 0.4553 - val_loss: 1.6567 - val_accuracy: 0.4133\n",
            "Epoch 25/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.5246 - accuracy: 0.5130 - val_loss: 1.6453 - val_accuracy: 0.4067\n",
            "Epoch 26/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.5284 - accuracy: 0.4691 - val_loss: 1.6331 - val_accuracy: 0.4133\n",
            "Epoch 27/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.5554 - accuracy: 0.4690 - val_loss: 1.6283 - val_accuracy: 0.4233\n",
            "Epoch 28/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4930 - accuracy: 0.5152 - val_loss: 1.6145 - val_accuracy: 0.4300\n",
            "Epoch 29/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.5102 - accuracy: 0.4961 - val_loss: 1.6115 - val_accuracy: 0.4300\n",
            "Epoch 30/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4671 - accuracy: 0.5251 - val_loss: 1.5996 - val_accuracy: 0.4300\n",
            "Epoch 31/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4545 - accuracy: 0.5355 - val_loss: 1.5956 - val_accuracy: 0.4233\n",
            "Epoch 32/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4290 - accuracy: 0.5300 - val_loss: 1.5832 - val_accuracy: 0.4333\n",
            "Epoch 33/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4933 - accuracy: 0.5113 - val_loss: 1.5771 - val_accuracy: 0.4633\n",
            "Epoch 34/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4611 - accuracy: 0.5192 - val_loss: 1.5697 - val_accuracy: 0.4600\n",
            "Epoch 35/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 1.4414 - accuracy: 0.5053 - val_loss: 1.5619 - val_accuracy: 0.4633\n",
            "Epoch 36/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4329 - accuracy: 0.4868 - val_loss: 1.5541 - val_accuracy: 0.4633\n",
            "Epoch 37/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4276 - accuracy: 0.5250 - val_loss: 1.5605 - val_accuracy: 0.4767\n",
            "Epoch 38/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.4062 - accuracy: 0.5205 - val_loss: 1.5446 - val_accuracy: 0.4400\n",
            "Epoch 39/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3894 - accuracy: 0.5410 - val_loss: 1.5433 - val_accuracy: 0.4767\n",
            "Epoch 40/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3707 - accuracy: 0.5453 - val_loss: 1.5353 - val_accuracy: 0.4867\n",
            "Epoch 41/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3448 - accuracy: 0.5625 - val_loss: 1.5264 - val_accuracy: 0.4667\n",
            "Epoch 42/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3734 - accuracy: 0.5270 - val_loss: 1.5227 - val_accuracy: 0.4900\n",
            "Epoch 43/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3648 - accuracy: 0.5216 - val_loss: 1.5169 - val_accuracy: 0.4833\n",
            "Epoch 44/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3598 - accuracy: 0.5630 - val_loss: 1.5146 - val_accuracy: 0.4833\n",
            "Epoch 45/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3418 - accuracy: 0.5187 - val_loss: 1.5089 - val_accuracy: 0.4900\n",
            "Epoch 46/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3391 - accuracy: 0.5438 - val_loss: 1.5106 - val_accuracy: 0.4767\n",
            "Epoch 47/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2757 - accuracy: 0.5837 - val_loss: 1.5032 - val_accuracy: 0.4867\n",
            "Epoch 48/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3034 - accuracy: 0.5719 - val_loss: 1.4986 - val_accuracy: 0.4933\n",
            "Epoch 49/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2597 - accuracy: 0.5720 - val_loss: 1.4951 - val_accuracy: 0.4967\n",
            "Epoch 50/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3229 - accuracy: 0.5429 - val_loss: 1.4913 - val_accuracy: 0.4833\n",
            "Epoch 51/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2849 - accuracy: 0.5812 - val_loss: 1.4910 - val_accuracy: 0.4600\n",
            "Epoch 52/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3256 - accuracy: 0.5290 - val_loss: 1.4881 - val_accuracy: 0.4633\n",
            "Epoch 53/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.3316 - accuracy: 0.5125 - val_loss: 1.4823 - val_accuracy: 0.4933\n",
            "Epoch 54/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2640 - accuracy: 0.5616 - val_loss: 1.4880 - val_accuracy: 0.4800\n",
            "Epoch 55/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2975 - accuracy: 0.5711 - val_loss: 1.4786 - val_accuracy: 0.4700\n",
            "Epoch 56/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2599 - accuracy: 0.5364 - val_loss: 1.4788 - val_accuracy: 0.4900\n",
            "Epoch 57/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2721 - accuracy: 0.5522 - val_loss: 1.4723 - val_accuracy: 0.4700\n",
            "Epoch 58/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2257 - accuracy: 0.5649 - val_loss: 1.4681 - val_accuracy: 0.4867\n",
            "Epoch 59/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2463 - accuracy: 0.5538 - val_loss: 1.4706 - val_accuracy: 0.4967\n",
            "Epoch 60/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2242 - accuracy: 0.5700 - val_loss: 1.4686 - val_accuracy: 0.4600\n",
            "Epoch 61/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2327 - accuracy: 0.5546 - val_loss: 1.4709 - val_accuracy: 0.4500\n",
            "Epoch 62/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2161 - accuracy: 0.5656 - val_loss: 1.4628 - val_accuracy: 0.4800\n",
            "Epoch 63/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2004 - accuracy: 0.5513 - val_loss: 1.4640 - val_accuracy: 0.4967\n",
            "Epoch 64/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2265 - accuracy: 0.5520 - val_loss: 1.4586 - val_accuracy: 0.4967\n",
            "Epoch 65/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2165 - accuracy: 0.5364 - val_loss: 1.4656 - val_accuracy: 0.4833\n",
            "Epoch 66/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2010 - accuracy: 0.5889 - val_loss: 1.4539 - val_accuracy: 0.5100\n",
            "Epoch 67/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.2059 - accuracy: 0.5697 - val_loss: 1.4568 - val_accuracy: 0.4867\n",
            "Epoch 68/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1314 - accuracy: 0.5843 - val_loss: 1.4524 - val_accuracy: 0.5000\n",
            "Epoch 69/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1692 - accuracy: 0.6096 - val_loss: 1.4513 - val_accuracy: 0.5100\n",
            "Epoch 70/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1898 - accuracy: 0.5524 - val_loss: 1.4514 - val_accuracy: 0.4833\n",
            "Epoch 71/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1623 - accuracy: 0.5927 - val_loss: 1.4503 - val_accuracy: 0.4633\n",
            "Epoch 72/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1902 - accuracy: 0.5651 - val_loss: 1.4507 - val_accuracy: 0.4833\n",
            "Epoch 73/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1274 - accuracy: 0.5922 - val_loss: 1.4500 - val_accuracy: 0.5033\n",
            "Epoch 74/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1593 - accuracy: 0.5971 - val_loss: 1.4457 - val_accuracy: 0.5000\n",
            "Epoch 75/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1935 - accuracy: 0.5699 - val_loss: 1.4540 - val_accuracy: 0.4800\n",
            "Epoch 76/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1750 - accuracy: 0.5839 - val_loss: 1.4507 - val_accuracy: 0.4767\n",
            "Epoch 77/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1515 - accuracy: 0.5685 - val_loss: 1.4446 - val_accuracy: 0.5000\n",
            "Epoch 78/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1431 - accuracy: 0.5951 - val_loss: 1.4442 - val_accuracy: 0.5100\n",
            "Epoch 79/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1270 - accuracy: 0.6147 - val_loss: 1.4455 - val_accuracy: 0.4900\n",
            "Epoch 80/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0909 - accuracy: 0.5939 - val_loss: 1.4434 - val_accuracy: 0.5233\n",
            "Epoch 81/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0771 - accuracy: 0.6180 - val_loss: 1.4462 - val_accuracy: 0.5100\n",
            "Epoch 82/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1422 - accuracy: 0.5992 - val_loss: 1.4474 - val_accuracy: 0.4967\n",
            "Epoch 83/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1292 - accuracy: 0.6068 - val_loss: 1.4533 - val_accuracy: 0.4567\n",
            "Epoch 84/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0655 - accuracy: 0.6165 - val_loss: 1.4541 - val_accuracy: 0.4733\n",
            "Epoch 85/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1384 - accuracy: 0.5963 - val_loss: 1.4479 - val_accuracy: 0.5067\n",
            "Epoch 86/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1370 - accuracy: 0.5770 - val_loss: 1.4490 - val_accuracy: 0.5233\n",
            "Epoch 87/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 1.1252 - accuracy: 0.6067 - val_loss: 1.4484 - val_accuracy: 0.4900\n",
            "Epoch 88/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0934 - accuracy: 0.6162 - val_loss: 1.4520 - val_accuracy: 0.5167\n",
            "Epoch 89/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0908 - accuracy: 0.5999 - val_loss: 1.4516 - val_accuracy: 0.5067\n",
            "Epoch 90/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0879 - accuracy: 0.6103 - val_loss: 1.4525 - val_accuracy: 0.5100\n",
            "Epoch 91/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0954 - accuracy: 0.6404 - val_loss: 1.4506 - val_accuracy: 0.5100\n",
            "Epoch 92/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0552 - accuracy: 0.6591 - val_loss: 1.4507 - val_accuracy: 0.5033\n",
            "Epoch 93/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0911 - accuracy: 0.6354 - val_loss: 1.4492 - val_accuracy: 0.5267\n",
            "Epoch 94/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0476 - accuracy: 0.6543 - val_loss: 1.4538 - val_accuracy: 0.5067\n",
            "Epoch 95/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0774 - accuracy: 0.6225 - val_loss: 1.4590 - val_accuracy: 0.5067\n",
            "Epoch 96/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0640 - accuracy: 0.6530 - val_loss: 1.4557 - val_accuracy: 0.5000\n",
            "Epoch 97/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0778 - accuracy: 0.6351 - val_loss: 1.4540 - val_accuracy: 0.5167\n",
            "Epoch 98/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0747 - accuracy: 0.6614 - val_loss: 1.4505 - val_accuracy: 0.5300\n",
            "Epoch 99/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0324 - accuracy: 0.6788 - val_loss: 1.4589 - val_accuracy: 0.5000\n",
            "Epoch 100/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0426 - accuracy: 0.6670 - val_loss: 1.4715 - val_accuracy: 0.4900\n",
            "Epoch 101/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.1011 - accuracy: 0.6274 - val_loss: 1.4548 - val_accuracy: 0.5300\n",
            "Epoch 102/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0449 - accuracy: 0.6512 - val_loss: 1.4543 - val_accuracy: 0.5333\n",
            "Epoch 103/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0276 - accuracy: 0.6519 - val_loss: 1.4612 - val_accuracy: 0.5267\n",
            "Epoch 104/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0351 - accuracy: 0.6558 - val_loss: 1.4598 - val_accuracy: 0.5200\n",
            "Epoch 105/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0242 - accuracy: 0.6571 - val_loss: 1.4589 - val_accuracy: 0.5267\n",
            "Epoch 106/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0078 - accuracy: 0.6821 - val_loss: 1.4589 - val_accuracy: 0.5333\n",
            "Epoch 107/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9975 - accuracy: 0.6703 - val_loss: 1.4715 - val_accuracy: 0.5067\n",
            "Epoch 108/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0283 - accuracy: 0.6600 - val_loss: 1.4628 - val_accuracy: 0.5267\n",
            "Epoch 109/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0484 - accuracy: 0.6494 - val_loss: 1.4832 - val_accuracy: 0.4967\n",
            "Epoch 110/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0200 - accuracy: 0.6685 - val_loss: 1.4672 - val_accuracy: 0.5200\n",
            "Epoch 111/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0346 - accuracy: 0.6610 - val_loss: 1.4720 - val_accuracy: 0.5067\n",
            "Epoch 112/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0030 - accuracy: 0.6736 - val_loss: 1.4713 - val_accuracy: 0.5300\n",
            "Epoch 113/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0297 - accuracy: 0.6763 - val_loss: 1.4708 - val_accuracy: 0.5233\n",
            "Epoch 114/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9837 - accuracy: 0.6970 - val_loss: 1.4672 - val_accuracy: 0.5133\n",
            "Epoch 115/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9985 - accuracy: 0.6761 - val_loss: 1.4741 - val_accuracy: 0.5200\n",
            "Epoch 116/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0087 - accuracy: 0.6642 - val_loss: 1.4797 - val_accuracy: 0.5067\n",
            "Epoch 117/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9782 - accuracy: 0.6800 - val_loss: 1.4756 - val_accuracy: 0.5200\n",
            "Epoch 118/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0267 - accuracy: 0.6649 - val_loss: 1.4870 - val_accuracy: 0.5033\n",
            "Epoch 119/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9948 - accuracy: 0.6701 - val_loss: 1.4839 - val_accuracy: 0.5267\n",
            "Epoch 120/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9482 - accuracy: 0.7035 - val_loss: 1.4821 - val_accuracy: 0.5200\n",
            "Epoch 121/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 1.0115 - accuracy: 0.6683 - val_loss: 1.4850 - val_accuracy: 0.5200\n",
            "Epoch 122/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9913 - accuracy: 0.6803 - val_loss: 1.4869 - val_accuracy: 0.5233\n",
            "Epoch 123/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9622 - accuracy: 0.6704 - val_loss: 1.4884 - val_accuracy: 0.5133\n",
            "Epoch 124/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9495 - accuracy: 0.6670 - val_loss: 1.4904 - val_accuracy: 0.5200\n",
            "Epoch 125/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9752 - accuracy: 0.6903 - val_loss: 1.4936 - val_accuracy: 0.5000\n",
            "Epoch 126/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9565 - accuracy: 0.6912 - val_loss: 1.4970 - val_accuracy: 0.5067\n",
            "Epoch 127/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9632 - accuracy: 0.6688 - val_loss: 1.5027 - val_accuracy: 0.5133\n",
            "Epoch 128/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9476 - accuracy: 0.6816 - val_loss: 1.5058 - val_accuracy: 0.5167\n",
            "Epoch 129/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9674 - accuracy: 0.6663 - val_loss: 1.5179 - val_accuracy: 0.4933\n",
            "Epoch 130/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.9613 - accuracy: 0.6996 - val_loss: 1.5056 - val_accuracy: 0.5133\n",
            "Epoch 131/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9635 - accuracy: 0.6650 - val_loss: 1.5056 - val_accuracy: 0.5200\n",
            "Epoch 132/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9313 - accuracy: 0.7214 - val_loss: 1.5072 - val_accuracy: 0.5100\n",
            "Epoch 133/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9712 - accuracy: 0.6891 - val_loss: 1.5103 - val_accuracy: 0.5233\n",
            "Epoch 134/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9081 - accuracy: 0.6750 - val_loss: 1.5137 - val_accuracy: 0.5133\n",
            "Epoch 135/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.6819 - val_loss: 1.5160 - val_accuracy: 0.5233\n",
            "Epoch 136/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9341 - accuracy: 0.7235 - val_loss: 1.5317 - val_accuracy: 0.5033\n",
            "Epoch 137/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9322 - accuracy: 0.7034 - val_loss: 1.5220 - val_accuracy: 0.5200\n",
            "Epoch 138/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9258 - accuracy: 0.6898 - val_loss: 1.5241 - val_accuracy: 0.5267\n",
            "Epoch 139/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9503 - accuracy: 0.6877 - val_loss: 1.5274 - val_accuracy: 0.5233\n",
            "Epoch 140/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9077 - accuracy: 0.6908 - val_loss: 1.5284 - val_accuracy: 0.5200\n",
            "Epoch 141/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9054 - accuracy: 0.6849 - val_loss: 1.5318 - val_accuracy: 0.5267\n",
            "Epoch 142/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9745 - accuracy: 0.7037 - val_loss: 1.5323 - val_accuracy: 0.5100\n",
            "Epoch 143/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9245 - accuracy: 0.6958 - val_loss: 1.5350 - val_accuracy: 0.5233\n",
            "Epoch 144/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9364 - accuracy: 0.7140 - val_loss: 1.5382 - val_accuracy: 0.5267\n",
            "Epoch 145/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8932 - accuracy: 0.7110 - val_loss: 1.5441 - val_accuracy: 0.5233\n",
            "Epoch 146/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9313 - accuracy: 0.7006 - val_loss: 1.5387 - val_accuracy: 0.5233\n",
            "Epoch 147/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8889 - accuracy: 0.7008 - val_loss: 1.5426 - val_accuracy: 0.5300\n",
            "Epoch 148/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8837 - accuracy: 0.7106 - val_loss: 1.5430 - val_accuracy: 0.5233\n",
            "Epoch 149/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9235 - accuracy: 0.7209 - val_loss: 1.5533 - val_accuracy: 0.5167\n",
            "Epoch 150/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8838 - accuracy: 0.6973 - val_loss: 1.5501 - val_accuracy: 0.5267\n",
            "Epoch 151/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9161 - accuracy: 0.7098 - val_loss: 1.5501 - val_accuracy: 0.5333\n",
            "Epoch 152/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8473 - accuracy: 0.7280 - val_loss: 1.5553 - val_accuracy: 0.5300\n",
            "Epoch 153/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8935 - accuracy: 0.6875 - val_loss: 1.5571 - val_accuracy: 0.5267\n",
            "Epoch 154/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9220 - accuracy: 0.7061 - val_loss: 1.5567 - val_accuracy: 0.5333\n",
            "Epoch 155/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8461 - accuracy: 0.7190 - val_loss: 1.5593 - val_accuracy: 0.5200\n",
            "Epoch 156/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9062 - accuracy: 0.7184 - val_loss: 1.5647 - val_accuracy: 0.5300\n",
            "Epoch 157/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.9424 - accuracy: 0.6839 - val_loss: 1.5648 - val_accuracy: 0.5233\n",
            "Epoch 158/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8972 - accuracy: 0.6804 - val_loss: 1.5670 - val_accuracy: 0.5300\n",
            "Epoch 159/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8686 - accuracy: 0.7039 - val_loss: 1.5696 - val_accuracy: 0.5200\n",
            "Epoch 160/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8904 - accuracy: 0.6805 - val_loss: 1.5748 - val_accuracy: 0.5200\n",
            "Epoch 161/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8636 - accuracy: 0.7378 - val_loss: 1.5765 - val_accuracy: 0.5200\n",
            "Epoch 162/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8590 - accuracy: 0.7388 - val_loss: 1.5777 - val_accuracy: 0.5167\n",
            "Epoch 163/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8381 - accuracy: 0.7034 - val_loss: 1.5798 - val_accuracy: 0.5233\n",
            "Epoch 164/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8405 - accuracy: 0.7601 - val_loss: 1.5803 - val_accuracy: 0.5267\n",
            "Epoch 165/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8478 - accuracy: 0.7248 - val_loss: 1.5827 - val_accuracy: 0.5200\n",
            "Epoch 166/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8689 - accuracy: 0.7204 - val_loss: 1.5871 - val_accuracy: 0.5233\n",
            "Epoch 167/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8386 - accuracy: 0.7697 - val_loss: 1.5890 - val_accuracy: 0.5200\n",
            "Epoch 168/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8739 - accuracy: 0.7067 - val_loss: 1.5897 - val_accuracy: 0.5167\n",
            "Epoch 169/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8641 - accuracy: 0.7153 - val_loss: 1.5874 - val_accuracy: 0.5167\n",
            "Epoch 170/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8681 - accuracy: 0.7007 - val_loss: 1.5906 - val_accuracy: 0.5267\n",
            "Epoch 171/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8446 - accuracy: 0.7159 - val_loss: 1.5977 - val_accuracy: 0.5233\n",
            "Epoch 172/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8289 - accuracy: 0.7323 - val_loss: 1.5970 - val_accuracy: 0.5267\n",
            "Epoch 173/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8555 - accuracy: 0.7260 - val_loss: 1.5969 - val_accuracy: 0.5300\n",
            "Epoch 174/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8359 - accuracy: 0.7297 - val_loss: 1.5997 - val_accuracy: 0.5233\n",
            "Epoch 175/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8430 - accuracy: 0.7236 - val_loss: 1.6045 - val_accuracy: 0.5200\n",
            "Epoch 176/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8486 - accuracy: 0.7266 - val_loss: 1.6035 - val_accuracy: 0.5233\n",
            "Epoch 177/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8559 - accuracy: 0.7337 - val_loss: 1.6096 - val_accuracy: 0.5233\n",
            "Epoch 178/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8669 - accuracy: 0.7198 - val_loss: 1.6048 - val_accuracy: 0.5267\n",
            "Epoch 179/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8502 - accuracy: 0.7111 - val_loss: 1.6061 - val_accuracy: 0.5233\n",
            "Epoch 180/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8231 - accuracy: 0.7413 - val_loss: 1.6056 - val_accuracy: 0.5167\n",
            "Epoch 181/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8626 - accuracy: 0.7120 - val_loss: 1.6129 - val_accuracy: 0.5167\n",
            "Epoch 182/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8315 - accuracy: 0.7067 - val_loss: 1.6134 - val_accuracy: 0.5233\n",
            "Epoch 183/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8051 - accuracy: 0.7446 - val_loss: 1.6111 - val_accuracy: 0.5367\n",
            "Epoch 184/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.8422 - accuracy: 0.7235 - val_loss: 1.6174 - val_accuracy: 0.5133\n",
            "Epoch 185/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8484 - accuracy: 0.7359 - val_loss: 1.6204 - val_accuracy: 0.5133\n",
            "Epoch 186/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8191 - accuracy: 0.7387 - val_loss: 1.6192 - val_accuracy: 0.5167\n",
            "Epoch 187/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7870 - accuracy: 0.7352 - val_loss: 1.6212 - val_accuracy: 0.5333\n",
            "Epoch 188/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8167 - accuracy: 0.7485 - val_loss: 1.6220 - val_accuracy: 0.5133\n",
            "Epoch 189/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8416 - accuracy: 0.7308 - val_loss: 1.6293 - val_accuracy: 0.5100\n",
            "Epoch 190/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7727 - accuracy: 0.7529 - val_loss: 1.6360 - val_accuracy: 0.5267\n",
            "Epoch 191/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8104 - accuracy: 0.7160 - val_loss: 1.6296 - val_accuracy: 0.5300\n",
            "Epoch 192/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8269 - accuracy: 0.7263 - val_loss: 1.6353 - val_accuracy: 0.5133\n",
            "Epoch 193/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7959 - accuracy: 0.7329 - val_loss: 1.6345 - val_accuracy: 0.5267\n",
            "Epoch 194/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7690 - accuracy: 0.7734 - val_loss: 1.6349 - val_accuracy: 0.5167\n",
            "Epoch 195/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7972 - accuracy: 0.7361 - val_loss: 1.6361 - val_accuracy: 0.5100\n",
            "Epoch 196/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8352 - accuracy: 0.7432 - val_loss: 1.6393 - val_accuracy: 0.5267\n",
            "Epoch 197/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7964 - accuracy: 0.7280 - val_loss: 1.6363 - val_accuracy: 0.5267\n",
            "Epoch 198/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7947 - accuracy: 0.7537 - val_loss: 1.6396 - val_accuracy: 0.5167\n",
            "Epoch 199/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7700 - accuracy: 0.7549 - val_loss: 1.6501 - val_accuracy: 0.5167\n",
            "Epoch 200/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7557 - accuracy: 0.7664 - val_loss: 1.6486 - val_accuracy: 0.5200\n",
            "Epoch 201/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8053 - accuracy: 0.7452 - val_loss: 1.6483 - val_accuracy: 0.5033\n",
            "Epoch 202/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7616 - accuracy: 0.7463 - val_loss: 1.6537 - val_accuracy: 0.5133\n",
            "Epoch 203/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7764 - accuracy: 0.7562 - val_loss: 1.6523 - val_accuracy: 0.5233\n",
            "Epoch 204/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7880 - accuracy: 0.7389 - val_loss: 1.6517 - val_accuracy: 0.5233\n",
            "Epoch 205/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7610 - accuracy: 0.7631 - val_loss: 1.6586 - val_accuracy: 0.5233\n",
            "Epoch 206/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7837 - accuracy: 0.7521 - val_loss: 1.6661 - val_accuracy: 0.5200\n",
            "Epoch 207/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8109 - accuracy: 0.7372 - val_loss: 1.6637 - val_accuracy: 0.5167\n",
            "Epoch 208/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7790 - accuracy: 0.7367 - val_loss: 1.6651 - val_accuracy: 0.5233\n",
            "Epoch 209/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8088 - accuracy: 0.7357 - val_loss: 1.6667 - val_accuracy: 0.5100\n",
            "Epoch 210/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7945 - accuracy: 0.7255 - val_loss: 1.6681 - val_accuracy: 0.5167\n",
            "Epoch 211/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7866 - accuracy: 0.7409 - val_loss: 1.6738 - val_accuracy: 0.5233\n",
            "Epoch 212/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7609 - accuracy: 0.7484 - val_loss: 1.6799 - val_accuracy: 0.5200\n",
            "Epoch 213/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7876 - accuracy: 0.7492 - val_loss: 1.6777 - val_accuracy: 0.5133\n",
            "Epoch 214/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8042 - accuracy: 0.7399 - val_loss: 1.6707 - val_accuracy: 0.5233\n",
            "Epoch 215/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7945 - accuracy: 0.7503 - val_loss: 1.6768 - val_accuracy: 0.5200\n",
            "Epoch 216/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7826 - accuracy: 0.7517 - val_loss: 1.6764 - val_accuracy: 0.5067\n",
            "Epoch 217/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8257 - accuracy: 0.7315 - val_loss: 1.6722 - val_accuracy: 0.5133\n",
            "Epoch 218/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7666 - accuracy: 0.7338 - val_loss: 1.6846 - val_accuracy: 0.5200\n",
            "Epoch 219/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7850 - accuracy: 0.7501 - val_loss: 1.6851 - val_accuracy: 0.5100\n",
            "Epoch 220/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7712 - accuracy: 0.7600 - val_loss: 1.6853 - val_accuracy: 0.5167\n",
            "Epoch 221/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7889 - accuracy: 0.7437 - val_loss: 1.6886 - val_accuracy: 0.5200\n",
            "Epoch 222/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7044 - accuracy: 0.7778 - val_loss: 1.6909 - val_accuracy: 0.5133\n",
            "Epoch 223/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7587 - accuracy: 0.7663 - val_loss: 1.7077 - val_accuracy: 0.5167\n",
            "Epoch 224/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8211 - accuracy: 0.7394 - val_loss: 1.7023 - val_accuracy: 0.5167\n",
            "Epoch 225/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7842 - accuracy: 0.7292 - val_loss: 1.7041 - val_accuracy: 0.5100\n",
            "Epoch 226/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.8081 - accuracy: 0.7253 - val_loss: 1.7009 - val_accuracy: 0.5167\n",
            "Epoch 227/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7310 - accuracy: 0.7721 - val_loss: 1.7027 - val_accuracy: 0.5033\n",
            "Epoch 228/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7583 - accuracy: 0.7443 - val_loss: 1.7102 - val_accuracy: 0.5233\n",
            "Epoch 229/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7356 - accuracy: 0.7424 - val_loss: 1.7024 - val_accuracy: 0.5167\n",
            "Epoch 230/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7361 - accuracy: 0.7822 - val_loss: 1.7070 - val_accuracy: 0.5200\n",
            "Epoch 231/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7252 - accuracy: 0.7653 - val_loss: 1.7073 - val_accuracy: 0.5067\n",
            "Epoch 232/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.8146 - accuracy: 0.7242 - val_loss: 1.7110 - val_accuracy: 0.5033\n",
            "Epoch 233/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7382 - accuracy: 0.7837 - val_loss: 1.7132 - val_accuracy: 0.5033\n",
            "Epoch 234/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7022 - accuracy: 0.7743 - val_loss: 1.7146 - val_accuracy: 0.5133\n",
            "Epoch 235/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7623 - accuracy: 0.7364 - val_loss: 1.7165 - val_accuracy: 0.5200\n",
            "Epoch 236/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7435 - accuracy: 0.7512 - val_loss: 1.7163 - val_accuracy: 0.5133\n",
            "Epoch 237/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7675 - accuracy: 0.7464 - val_loss: 1.7176 - val_accuracy: 0.5100\n",
            "Epoch 238/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7279 - accuracy: 0.7693 - val_loss: 1.7341 - val_accuracy: 0.5167\n",
            "Epoch 239/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7622 - accuracy: 0.7473 - val_loss: 1.7261 - val_accuracy: 0.5200\n",
            "Epoch 240/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7220 - accuracy: 0.7812 - val_loss: 1.7232 - val_accuracy: 0.5033\n",
            "Epoch 241/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7087 - accuracy: 0.7718 - val_loss: 1.7297 - val_accuracy: 0.5100\n",
            "Epoch 242/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7317 - accuracy: 0.7512 - val_loss: 1.7287 - val_accuracy: 0.5100\n",
            "Epoch 243/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7529 - accuracy: 0.7615 - val_loss: 1.7402 - val_accuracy: 0.5200\n",
            "Epoch 244/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7095 - accuracy: 0.7638 - val_loss: 1.7286 - val_accuracy: 0.5167\n",
            "Epoch 245/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7506 - accuracy: 0.7769 - val_loss: 1.7431 - val_accuracy: 0.5133\n",
            "Epoch 246/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.7857 - val_loss: 1.7430 - val_accuracy: 0.5167\n",
            "Epoch 247/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7671 - accuracy: 0.7465 - val_loss: 1.7401 - val_accuracy: 0.5133\n",
            "Epoch 248/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.7583 - val_loss: 1.7482 - val_accuracy: 0.5100\n",
            "Epoch 249/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7644 - accuracy: 0.7591 - val_loss: 1.7446 - val_accuracy: 0.5167\n",
            "Epoch 250/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.7619 - val_loss: 1.7487 - val_accuracy: 0.5233\n",
            "Epoch 251/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7283 - accuracy: 0.7352 - val_loss: 1.7456 - val_accuracy: 0.5033\n",
            "Epoch 252/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7332 - accuracy: 0.7652 - val_loss: 1.7582 - val_accuracy: 0.5200\n",
            "Epoch 253/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7409 - accuracy: 0.7696 - val_loss: 1.7615 - val_accuracy: 0.5033\n",
            "Epoch 254/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7819 - accuracy: 0.7144 - val_loss: 1.7599 - val_accuracy: 0.5200\n",
            "Epoch 255/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7418 - accuracy: 0.7534 - val_loss: 1.7589 - val_accuracy: 0.5167\n",
            "Epoch 256/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7297 - accuracy: 0.7857 - val_loss: 1.7565 - val_accuracy: 0.5100\n",
            "Epoch 257/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7245 - accuracy: 0.7505 - val_loss: 1.7567 - val_accuracy: 0.5167\n",
            "Epoch 258/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7275 - accuracy: 0.7548 - val_loss: 1.7656 - val_accuracy: 0.5133\n",
            "Epoch 259/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7487 - accuracy: 0.7682 - val_loss: 1.7708 - val_accuracy: 0.5167\n",
            "Epoch 260/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6968 - accuracy: 0.7677 - val_loss: 1.7671 - val_accuracy: 0.5267\n",
            "Epoch 261/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6991 - accuracy: 0.7880 - val_loss: 1.7731 - val_accuracy: 0.5133\n",
            "Epoch 262/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7543 - accuracy: 0.7551 - val_loss: 1.7704 - val_accuracy: 0.5200\n",
            "Epoch 263/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7173 - accuracy: 0.7731 - val_loss: 1.7789 - val_accuracy: 0.5033\n",
            "Epoch 264/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7222 - accuracy: 0.7602 - val_loss: 1.7809 - val_accuracy: 0.5233\n",
            "Epoch 265/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7000 - accuracy: 0.7916 - val_loss: 1.7775 - val_accuracy: 0.5200\n",
            "Epoch 266/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7263 - accuracy: 0.7502 - val_loss: 1.7831 - val_accuracy: 0.5267\n",
            "Epoch 267/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.7681 - val_loss: 1.7691 - val_accuracy: 0.5267\n",
            "Epoch 268/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7180 - accuracy: 0.7721 - val_loss: 1.7840 - val_accuracy: 0.5200\n",
            "Epoch 269/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7591 - accuracy: 0.7326 - val_loss: 1.7808 - val_accuracy: 0.5200\n",
            "Epoch 270/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7049 - accuracy: 0.7806 - val_loss: 1.7803 - val_accuracy: 0.5167\n",
            "Epoch 271/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.7598 - val_loss: 1.7900 - val_accuracy: 0.5200\n",
            "Epoch 272/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.8130 - val_loss: 1.7839 - val_accuracy: 0.5200\n",
            "Epoch 273/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7013 - accuracy: 0.7769 - val_loss: 1.7900 - val_accuracy: 0.5133\n",
            "Epoch 274/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7436 - accuracy: 0.7659 - val_loss: 1.7942 - val_accuracy: 0.5200\n",
            "Epoch 275/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7682 - accuracy: 0.7821 - val_loss: 1.7890 - val_accuracy: 0.5133\n",
            "Epoch 276/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6734 - accuracy: 0.7790 - val_loss: 1.8010 - val_accuracy: 0.5267\n",
            "Epoch 277/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7238 - accuracy: 0.7642 - val_loss: 1.8023 - val_accuracy: 0.5133\n",
            "Epoch 278/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6773 - accuracy: 0.7868 - val_loss: 1.7972 - val_accuracy: 0.5200\n",
            "Epoch 279/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.7708 - val_loss: 1.8033 - val_accuracy: 0.5100\n",
            "Epoch 280/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.7706 - val_loss: 1.8025 - val_accuracy: 0.5167\n",
            "Epoch 281/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7061 - accuracy: 0.7567 - val_loss: 1.8045 - val_accuracy: 0.5133\n",
            "Epoch 282/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7026 - accuracy: 0.7686 - val_loss: 1.8062 - val_accuracy: 0.5133\n",
            "Epoch 283/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7122 - accuracy: 0.7767 - val_loss: 1.8169 - val_accuracy: 0.5133\n",
            "Epoch 284/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7079 - accuracy: 0.7636 - val_loss: 1.8108 - val_accuracy: 0.5200\n",
            "Epoch 285/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.7689 - val_loss: 1.8118 - val_accuracy: 0.5200\n",
            "Epoch 286/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7024 - accuracy: 0.7596 - val_loss: 1.8161 - val_accuracy: 0.5200\n",
            "Epoch 287/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6693 - accuracy: 0.7868 - val_loss: 1.8213 - val_accuracy: 0.5233\n",
            "Epoch 288/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.7849 - val_loss: 1.8204 - val_accuracy: 0.5300\n",
            "Epoch 289/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6994 - accuracy: 0.7867 - val_loss: 1.8220 - val_accuracy: 0.5367\n",
            "Epoch 290/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6991 - accuracy: 0.7867 - val_loss: 1.8189 - val_accuracy: 0.5333\n",
            "Epoch 291/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.7906 - val_loss: 1.8294 - val_accuracy: 0.5300\n",
            "Epoch 292/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.7978 - val_loss: 1.8304 - val_accuracy: 0.5400\n",
            "Epoch 293/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.7667 - val_loss: 1.8326 - val_accuracy: 0.5333\n",
            "Epoch 294/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.8069 - val_loss: 1.8333 - val_accuracy: 0.5300\n",
            "Epoch 295/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.7888 - val_loss: 1.8325 - val_accuracy: 0.5367\n",
            "Epoch 296/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.7996 - val_loss: 1.8344 - val_accuracy: 0.5367\n",
            "Epoch 297/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.7674 - val_loss: 1.8385 - val_accuracy: 0.5433\n",
            "Epoch 298/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.7806 - val_loss: 1.8408 - val_accuracy: 0.5367\n",
            "Epoch 299/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6667 - accuracy: 0.8076 - val_loss: 1.8435 - val_accuracy: 0.5300\n",
            "Epoch 300/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.7950 - val_loss: 1.8463 - val_accuracy: 0.5367\n",
            "Epoch 301/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6688 - accuracy: 0.7948 - val_loss: 1.8384 - val_accuracy: 0.5333\n",
            "Epoch 302/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6621 - accuracy: 0.7926 - val_loss: 1.8563 - val_accuracy: 0.5400\n",
            "Epoch 303/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.7894 - val_loss: 1.8444 - val_accuracy: 0.5400\n",
            "Epoch 304/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.8054 - val_loss: 1.8516 - val_accuracy: 0.5333\n",
            "Epoch 305/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.8036 - val_loss: 1.8556 - val_accuracy: 0.5300\n",
            "Epoch 306/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.8214 - val_loss: 1.8576 - val_accuracy: 0.5400\n",
            "Epoch 307/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.8139 - val_loss: 1.8501 - val_accuracy: 0.5367\n",
            "Epoch 308/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.7017 - accuracy: 0.7806 - val_loss: 1.8547 - val_accuracy: 0.5367\n",
            "Epoch 309/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.7979 - val_loss: 1.8639 - val_accuracy: 0.5367\n",
            "Epoch 310/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6687 - accuracy: 0.7938 - val_loss: 1.8614 - val_accuracy: 0.5367\n",
            "Epoch 311/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.7874 - val_loss: 1.8643 - val_accuracy: 0.5367\n",
            "Epoch 312/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.7967 - val_loss: 1.8701 - val_accuracy: 0.5300\n",
            "Epoch 313/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6621 - accuracy: 0.8139 - val_loss: 1.8500 - val_accuracy: 0.5400\n",
            "Epoch 314/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.7798 - val_loss: 1.8785 - val_accuracy: 0.5400\n",
            "Epoch 315/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6600 - accuracy: 0.7916 - val_loss: 1.8740 - val_accuracy: 0.5367\n",
            "Epoch 316/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.7883 - val_loss: 1.8652 - val_accuracy: 0.5400\n",
            "Epoch 317/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.7954 - val_loss: 1.8746 - val_accuracy: 0.5367\n",
            "Epoch 318/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6613 - accuracy: 0.7910 - val_loss: 1.8737 - val_accuracy: 0.5233\n",
            "Epoch 319/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.7985 - val_loss: 1.8699 - val_accuracy: 0.5333\n",
            "Epoch 320/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.6784 - accuracy: 0.8144 - val_loss: 1.8857 - val_accuracy: 0.5400\n",
            "Epoch 321/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.7781 - val_loss: 1.8853 - val_accuracy: 0.5400\n",
            "Epoch 322/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6553 - accuracy: 0.8061 - val_loss: 1.8840 - val_accuracy: 0.5300\n",
            "Epoch 323/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6410 - accuracy: 0.8022 - val_loss: 1.8796 - val_accuracy: 0.5367\n",
            "Epoch 324/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6697 - accuracy: 0.8062 - val_loss: 1.8870 - val_accuracy: 0.5533\n",
            "Epoch 325/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.7943 - val_loss: 1.8983 - val_accuracy: 0.5400\n",
            "Epoch 326/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6336 - accuracy: 0.8148 - val_loss: 1.8990 - val_accuracy: 0.5400\n",
            "Epoch 327/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6771 - accuracy: 0.7919 - val_loss: 1.8854 - val_accuracy: 0.5333\n",
            "Epoch 328/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6615 - accuracy: 0.8239 - val_loss: 1.8923 - val_accuracy: 0.5467\n",
            "Epoch 329/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6223 - accuracy: 0.8236 - val_loss: 1.9158 - val_accuracy: 0.5400\n",
            "Epoch 330/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.8132 - val_loss: 1.8949 - val_accuracy: 0.5467\n",
            "Epoch 331/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.8151 - val_loss: 1.8990 - val_accuracy: 0.5400\n",
            "Epoch 332/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.8032 - val_loss: 1.8977 - val_accuracy: 0.5367\n",
            "Epoch 333/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.8120 - val_loss: 1.9017 - val_accuracy: 0.5400\n",
            "Epoch 334/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6753 - accuracy: 0.7800 - val_loss: 1.9224 - val_accuracy: 0.5400\n",
            "Epoch 335/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6578 - accuracy: 0.8038 - val_loss: 1.9065 - val_accuracy: 0.5433\n",
            "Epoch 336/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6107 - accuracy: 0.8001 - val_loss: 1.9109 - val_accuracy: 0.5400\n",
            "Epoch 337/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.7978 - val_loss: 1.9121 - val_accuracy: 0.5433\n",
            "Epoch 338/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.7934 - val_loss: 1.9170 - val_accuracy: 0.5433\n",
            "Epoch 339/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.8318 - val_loss: 1.9150 - val_accuracy: 0.5467\n",
            "Epoch 340/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.8116 - val_loss: 1.9171 - val_accuracy: 0.5433\n",
            "Epoch 341/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.8276 - val_loss: 1.9270 - val_accuracy: 0.5400\n",
            "Epoch 342/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.8070 - val_loss: 1.9126 - val_accuracy: 0.5400\n",
            "Epoch 343/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.8109 - val_loss: 1.9254 - val_accuracy: 0.5400\n",
            "Epoch 344/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.8148 - val_loss: 1.9206 - val_accuracy: 0.5467\n",
            "Epoch 345/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.8367 - val_loss: 1.9303 - val_accuracy: 0.5433\n",
            "Epoch 346/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.8111 - val_loss: 1.9233 - val_accuracy: 0.5400\n",
            "Epoch 347/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6553 - accuracy: 0.8371 - val_loss: 1.9330 - val_accuracy: 0.5433\n",
            "Epoch 348/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.7997 - val_loss: 1.9284 - val_accuracy: 0.5467\n",
            "Epoch 349/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.8145 - val_loss: 1.9333 - val_accuracy: 0.5433\n",
            "Epoch 350/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.8193 - val_loss: 1.9167 - val_accuracy: 0.5433\n",
            "Epoch 351/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.8250 - val_loss: 1.9345 - val_accuracy: 0.5367\n",
            "Epoch 352/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.8215 - val_loss: 1.9266 - val_accuracy: 0.5467\n",
            "Epoch 353/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.7962 - val_loss: 1.9406 - val_accuracy: 0.5400\n",
            "Epoch 354/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6254 - accuracy: 0.8223 - val_loss: 1.9400 - val_accuracy: 0.5500\n",
            "Epoch 355/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.7914 - val_loss: 1.9458 - val_accuracy: 0.5367\n",
            "Epoch 356/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6789 - accuracy: 0.8089 - val_loss: 1.9428 - val_accuracy: 0.5433\n",
            "Epoch 357/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6113 - accuracy: 0.8155 - val_loss: 1.9477 - val_accuracy: 0.5433\n",
            "Epoch 358/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.8173 - val_loss: 1.9576 - val_accuracy: 0.5467\n",
            "Epoch 359/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.8026 - val_loss: 1.9508 - val_accuracy: 0.5533\n",
            "Epoch 360/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5919 - accuracy: 0.8210 - val_loss: 1.9482 - val_accuracy: 0.5467\n",
            "Epoch 361/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.8290 - val_loss: 1.9590 - val_accuracy: 0.5433\n",
            "Epoch 362/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.8169 - val_loss: 1.9533 - val_accuracy: 0.5433\n",
            "Epoch 363/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.8126 - val_loss: 1.9582 - val_accuracy: 0.5467\n",
            "Epoch 364/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.8123 - val_loss: 1.9679 - val_accuracy: 0.5433\n",
            "Epoch 365/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6492 - accuracy: 0.8008 - val_loss: 1.9598 - val_accuracy: 0.5467\n",
            "Epoch 366/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.8486 - val_loss: 1.9571 - val_accuracy: 0.5467\n",
            "Epoch 367/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.8265 - val_loss: 1.9628 - val_accuracy: 0.5500\n",
            "Epoch 368/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.8336 - val_loss: 1.9765 - val_accuracy: 0.5367\n",
            "Epoch 369/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5994 - accuracy: 0.8332 - val_loss: 1.9680 - val_accuracy: 0.5400\n",
            "Epoch 370/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.8223 - val_loss: 1.9613 - val_accuracy: 0.5433\n",
            "Epoch 371/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.8415 - val_loss: 1.9683 - val_accuracy: 0.5367\n",
            "Epoch 372/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5666 - accuracy: 0.8268 - val_loss: 1.9671 - val_accuracy: 0.5467\n",
            "Epoch 373/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.8321 - val_loss: 1.9503 - val_accuracy: 0.5500\n",
            "Epoch 374/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.8086 - val_loss: 1.9758 - val_accuracy: 0.5400\n",
            "Epoch 375/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.8116 - val_loss: 1.9609 - val_accuracy: 0.5467\n",
            "Epoch 376/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.8236 - val_loss: 1.9738 - val_accuracy: 0.5500\n",
            "Epoch 377/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.8117 - val_loss: 1.9754 - val_accuracy: 0.5433\n",
            "Epoch 378/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.8334 - val_loss: 1.9734 - val_accuracy: 0.5533\n",
            "Epoch 379/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.8009 - val_loss: 1.9645 - val_accuracy: 0.5467\n",
            "Epoch 380/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.8335 - val_loss: 1.9725 - val_accuracy: 0.5533\n",
            "Epoch 381/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6097 - accuracy: 0.8088 - val_loss: 1.9742 - val_accuracy: 0.5400\n",
            "Epoch 382/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.8148 - val_loss: 1.9742 - val_accuracy: 0.5500\n",
            "Epoch 383/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.8557 - val_loss: 1.9794 - val_accuracy: 0.5467\n",
            "Epoch 384/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6035 - accuracy: 0.8370 - val_loss: 1.9935 - val_accuracy: 0.5433\n",
            "Epoch 385/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6227 - accuracy: 0.8207 - val_loss: 1.9801 - val_accuracy: 0.5500\n",
            "Epoch 386/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.8279 - val_loss: 2.0001 - val_accuracy: 0.5500\n",
            "Epoch 387/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.8190 - val_loss: 2.0022 - val_accuracy: 0.5533\n",
            "Epoch 388/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.8324 - val_loss: 2.0087 - val_accuracy: 0.5500\n",
            "Epoch 389/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.8491 - val_loss: 1.9913 - val_accuracy: 0.5433\n",
            "Epoch 390/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5870 - accuracy: 0.8396 - val_loss: 1.9911 - val_accuracy: 0.5500\n",
            "Epoch 391/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.8371 - val_loss: 1.9990 - val_accuracy: 0.5467\n",
            "Epoch 392/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.8213 - val_loss: 1.9950 - val_accuracy: 0.5500\n",
            "Epoch 393/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6343 - accuracy: 0.8232 - val_loss: 1.9950 - val_accuracy: 0.5567\n",
            "Epoch 394/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5959 - accuracy: 0.8238 - val_loss: 1.9866 - val_accuracy: 0.5500\n",
            "Epoch 395/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5843 - accuracy: 0.8307 - val_loss: 1.9869 - val_accuracy: 0.5433\n",
            "Epoch 396/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.8138 - val_loss: 2.0152 - val_accuracy: 0.5233\n",
            "Epoch 397/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.8072 - val_loss: 2.0028 - val_accuracy: 0.5433\n",
            "Epoch 398/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.8109 - val_loss: 2.0029 - val_accuracy: 0.5500\n",
            "Epoch 399/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.8306 - val_loss: 2.0090 - val_accuracy: 0.5533\n",
            "Epoch 400/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.8346 - val_loss: 2.0080 - val_accuracy: 0.5567\n",
            "Epoch 401/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.8185 - val_loss: 2.0091 - val_accuracy: 0.5567\n",
            "Epoch 402/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.8454 - val_loss: 2.0144 - val_accuracy: 0.5300\n",
            "Epoch 403/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.5773 - accuracy: 0.8340 - val_loss: 2.0119 - val_accuracy: 0.5500\n",
            "Epoch 404/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.8413 - val_loss: 2.0201 - val_accuracy: 0.5500\n",
            "Epoch 405/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.8233 - val_loss: 2.0092 - val_accuracy: 0.5400\n",
            "Epoch 406/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.8178 - val_loss: 2.0149 - val_accuracy: 0.5533\n",
            "Epoch 407/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.8263 - val_loss: 2.0242 - val_accuracy: 0.5533\n",
            "Epoch 408/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.8256 - val_loss: 2.0118 - val_accuracy: 0.5467\n",
            "Epoch 409/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5728 - accuracy: 0.8334 - val_loss: 2.0199 - val_accuracy: 0.5400\n",
            "Epoch 410/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.8216 - val_loss: 2.0280 - val_accuracy: 0.5500\n",
            "Epoch 411/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5962 - accuracy: 0.8310 - val_loss: 2.0182 - val_accuracy: 0.5400\n",
            "Epoch 412/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.8520 - val_loss: 2.0173 - val_accuracy: 0.5433\n",
            "Epoch 413/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.8645 - val_loss: 2.0288 - val_accuracy: 0.5500\n",
            "Epoch 414/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.8427 - val_loss: 2.0257 - val_accuracy: 0.5500\n",
            "Epoch 415/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5924 - accuracy: 0.8253 - val_loss: 2.0302 - val_accuracy: 0.5433\n",
            "Epoch 416/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.8423 - val_loss: 2.0421 - val_accuracy: 0.5233\n",
            "Epoch 417/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.8414 - val_loss: 2.0435 - val_accuracy: 0.5467\n",
            "Epoch 418/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.8217 - val_loss: 2.0212 - val_accuracy: 0.5433\n",
            "Epoch 419/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.8207 - val_loss: 2.0438 - val_accuracy: 0.5500\n",
            "Epoch 420/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.8257 - val_loss: 2.0488 - val_accuracy: 0.5400\n",
            "Epoch 421/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.8368 - val_loss: 2.0525 - val_accuracy: 0.5433\n",
            "Epoch 422/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.8360 - val_loss: 2.0479 - val_accuracy: 0.5433\n",
            "Epoch 423/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.8274 - val_loss: 2.0702 - val_accuracy: 0.5433\n",
            "Epoch 424/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.8408 - val_loss: 2.0360 - val_accuracy: 0.5267\n",
            "Epoch 425/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5444 - accuracy: 0.8597 - val_loss: 2.0807 - val_accuracy: 0.5467\n",
            "Epoch 426/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.8359 - val_loss: 2.0617 - val_accuracy: 0.5233\n",
            "Epoch 427/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.8482 - val_loss: 2.0700 - val_accuracy: 0.5400\n",
            "Epoch 428/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.8631 - val_loss: 2.0664 - val_accuracy: 0.5367\n",
            "Epoch 429/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.8249 - val_loss: 2.0389 - val_accuracy: 0.5333\n",
            "Epoch 430/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.8620 - val_loss: 2.0634 - val_accuracy: 0.5467\n",
            "Epoch 431/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.8254 - val_loss: 2.0700 - val_accuracy: 0.5367\n",
            "Epoch 432/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.8282 - val_loss: 2.0785 - val_accuracy: 0.5433\n",
            "Epoch 433/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.8455 - val_loss: 2.0746 - val_accuracy: 0.5400\n",
            "Epoch 434/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.8218 - val_loss: 2.0611 - val_accuracy: 0.5433\n",
            "Epoch 435/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.8421 - val_loss: 2.0706 - val_accuracy: 0.5267\n",
            "Epoch 436/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.8411 - val_loss: 2.0729 - val_accuracy: 0.5300\n",
            "Epoch 437/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.8268 - val_loss: 2.0561 - val_accuracy: 0.5300\n",
            "Epoch 438/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.8506 - val_loss: 2.0714 - val_accuracy: 0.5433\n",
            "Epoch 439/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.8268 - val_loss: 2.0759 - val_accuracy: 0.5400\n",
            "Epoch 440/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.8118 - val_loss: 2.0809 - val_accuracy: 0.5400\n",
            "Epoch 441/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.8325 - val_loss: 2.0740 - val_accuracy: 0.5467\n",
            "Epoch 442/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.8235 - val_loss: 2.0920 - val_accuracy: 0.5400\n",
            "Epoch 443/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.8359 - val_loss: 2.0880 - val_accuracy: 0.5433\n",
            "Epoch 444/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.8470 - val_loss: 2.0864 - val_accuracy: 0.5300\n",
            "Epoch 445/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5267 - accuracy: 0.8551 - val_loss: 2.0892 - val_accuracy: 0.5433\n",
            "Epoch 446/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.8246 - val_loss: 2.0814 - val_accuracy: 0.5367\n",
            "Epoch 447/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.8539 - val_loss: 2.0928 - val_accuracy: 0.5433\n",
            "Epoch 448/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.8404 - val_loss: 2.1008 - val_accuracy: 0.5400\n",
            "Epoch 449/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5989 - accuracy: 0.8161 - val_loss: 2.1049 - val_accuracy: 0.5333\n",
            "Epoch 450/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.8403 - val_loss: 2.0980 - val_accuracy: 0.5433\n",
            "Epoch 451/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.8796 - val_loss: 2.0958 - val_accuracy: 0.5433\n",
            "Epoch 452/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.8443 - val_loss: 2.1160 - val_accuracy: 0.5433\n",
            "Epoch 453/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.8369 - val_loss: 2.1131 - val_accuracy: 0.5433\n",
            "Epoch 454/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.8315 - val_loss: 2.1033 - val_accuracy: 0.5400\n",
            "Epoch 455/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.8510 - val_loss: 2.1093 - val_accuracy: 0.5367\n",
            "Epoch 456/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5944 - accuracy: 0.8050 - val_loss: 2.1179 - val_accuracy: 0.5433\n",
            "Epoch 457/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.8289 - val_loss: 2.1160 - val_accuracy: 0.5433\n",
            "Epoch 458/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.8623 - val_loss: 2.1187 - val_accuracy: 0.5367\n",
            "Epoch 459/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.8281 - val_loss: 2.1095 - val_accuracy: 0.5433\n",
            "Epoch 460/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.8501 - val_loss: 2.1185 - val_accuracy: 0.5400\n",
            "Epoch 461/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5407 - accuracy: 0.8528 - val_loss: 2.1245 - val_accuracy: 0.5400\n",
            "Epoch 462/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.8560 - val_loss: 2.1222 - val_accuracy: 0.5367\n",
            "Epoch 463/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.8776 - val_loss: 2.1174 - val_accuracy: 0.5400\n",
            "Epoch 464/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5757 - accuracy: 0.8303 - val_loss: 2.1136 - val_accuracy: 0.5467\n",
            "Epoch 465/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.8452 - val_loss: 2.1209 - val_accuracy: 0.5333\n",
            "Epoch 466/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.8324 - val_loss: 2.1342 - val_accuracy: 0.5500\n",
            "Epoch 467/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.8713 - val_loss: 2.1223 - val_accuracy: 0.5333\n",
            "Epoch 468/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5483 - accuracy: 0.8579 - val_loss: 2.1368 - val_accuracy: 0.5433\n",
            "Epoch 469/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.8552 - val_loss: 2.1239 - val_accuracy: 0.5367\n",
            "Epoch 470/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.8469 - val_loss: 2.1365 - val_accuracy: 0.5333\n",
            "Epoch 471/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.8174 - val_loss: 2.1542 - val_accuracy: 0.5433\n",
            "Epoch 472/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5375 - accuracy: 0.8527 - val_loss: 2.1472 - val_accuracy: 0.5400\n",
            "Epoch 473/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.8601 - val_loss: 2.1348 - val_accuracy: 0.5400\n",
            "Epoch 474/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5694 - accuracy: 0.8353 - val_loss: 2.1476 - val_accuracy: 0.5300\n",
            "Epoch 475/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.8589 - val_loss: 2.1447 - val_accuracy: 0.5400\n",
            "Epoch 476/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.8405 - val_loss: 2.1393 - val_accuracy: 0.5333\n",
            "Epoch 477/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.8410 - val_loss: 2.1396 - val_accuracy: 0.5333\n",
            "Epoch 478/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.8470 - val_loss: 2.1428 - val_accuracy: 0.5400\n",
            "Epoch 479/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.8482 - val_loss: 2.1541 - val_accuracy: 0.5433\n",
            "Epoch 480/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.8883 - val_loss: 2.1440 - val_accuracy: 0.5367\n",
            "Epoch 481/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5469 - accuracy: 0.8369 - val_loss: 2.1470 - val_accuracy: 0.5433\n",
            "Epoch 482/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.8571 - val_loss: 2.1431 - val_accuracy: 0.5367\n",
            "Epoch 483/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.8490 - val_loss: 2.1303 - val_accuracy: 0.5333\n",
            "Epoch 484/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.8335 - val_loss: 2.1469 - val_accuracy: 0.5367\n",
            "Epoch 485/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.8652 - val_loss: 2.1630 - val_accuracy: 0.5433\n",
            "Epoch 486/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.8511 - val_loss: 2.1630 - val_accuracy: 0.5300\n",
            "Epoch 487/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.8810 - val_loss: 2.1663 - val_accuracy: 0.5300\n",
            "Epoch 488/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.8622 - val_loss: 2.1633 - val_accuracy: 0.5400\n",
            "Epoch 489/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.8554 - val_loss: 2.1873 - val_accuracy: 0.5367\n",
            "Epoch 490/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.8815 - val_loss: 2.1659 - val_accuracy: 0.5333\n",
            "Epoch 491/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.8622 - val_loss: 2.1821 - val_accuracy: 0.5467\n",
            "Epoch 492/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.8535 - val_loss: 2.1826 - val_accuracy: 0.5500\n",
            "Epoch 493/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.8625 - val_loss: 2.1768 - val_accuracy: 0.5333\n",
            "Epoch 494/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.8609 - val_loss: 2.1690 - val_accuracy: 0.5267\n",
            "Epoch 495/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.8575 - val_loss: 2.1676 - val_accuracy: 0.5333\n",
            "Epoch 496/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.8180 - val_loss: 2.1740 - val_accuracy: 0.5267\n",
            "Epoch 497/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.8625 - val_loss: 2.1632 - val_accuracy: 0.5300\n",
            "Epoch 498/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5810 - accuracy: 0.8348 - val_loss: 2.1799 - val_accuracy: 0.5300\n",
            "Epoch 499/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.8468 - val_loss: 2.1918 - val_accuracy: 0.5267\n",
            "Epoch 500/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.8718 - val_loss: 2.1842 - val_accuracy: 0.5333\n",
            "Epoch 501/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.8622 - val_loss: 2.1961 - val_accuracy: 0.5367\n",
            "Epoch 502/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.8760 - val_loss: 2.1896 - val_accuracy: 0.5433\n",
            "Epoch 503/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.8566 - val_loss: 2.1944 - val_accuracy: 0.5367\n",
            "Epoch 504/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.8352 - val_loss: 2.1786 - val_accuracy: 0.5367\n",
            "Epoch 505/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.8639 - val_loss: 2.1985 - val_accuracy: 0.5267\n",
            "Epoch 506/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.8535 - val_loss: 2.1929 - val_accuracy: 0.5367\n",
            "Epoch 507/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.8764 - val_loss: 2.2005 - val_accuracy: 0.5300\n",
            "Epoch 508/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.8644 - val_loss: 2.2156 - val_accuracy: 0.5367\n",
            "Epoch 509/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.8267 - val_loss: 2.2174 - val_accuracy: 0.5333\n",
            "Epoch 510/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.8675 - val_loss: 2.2104 - val_accuracy: 0.5333\n",
            "Epoch 511/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5304 - accuracy: 0.8633 - val_loss: 2.2156 - val_accuracy: 0.5400\n",
            "Epoch 512/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.8636 - val_loss: 2.1996 - val_accuracy: 0.5300\n",
            "Epoch 513/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.8383 - val_loss: 2.2327 - val_accuracy: 0.5300\n",
            "Epoch 514/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.8573 - val_loss: 2.2104 - val_accuracy: 0.5233\n",
            "Epoch 515/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.8733 - val_loss: 2.2238 - val_accuracy: 0.5233\n",
            "Epoch 516/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.8452 - val_loss: 2.2300 - val_accuracy: 0.5167\n",
            "Epoch 517/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.8557 - val_loss: 2.2370 - val_accuracy: 0.5233\n",
            "Epoch 518/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.8415 - val_loss: 2.2326 - val_accuracy: 0.5300\n",
            "Epoch 519/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.8372 - val_loss: 2.2333 - val_accuracy: 0.5333\n",
            "Epoch 520/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.8681 - val_loss: 2.2471 - val_accuracy: 0.5233\n",
            "Epoch 521/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.8601 - val_loss: 2.2324 - val_accuracy: 0.5300\n",
            "Epoch 522/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.8490 - val_loss: 2.2378 - val_accuracy: 0.5233\n",
            "Epoch 523/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.8532 - val_loss: 2.2447 - val_accuracy: 0.5400\n",
            "Epoch 524/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8623 - val_loss: 2.2239 - val_accuracy: 0.5333\n",
            "Epoch 525/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.8633 - val_loss: 2.2428 - val_accuracy: 0.5167\n",
            "Epoch 526/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.8611 - val_loss: 2.2371 - val_accuracy: 0.5367\n",
            "Epoch 527/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.8696 - val_loss: 2.2427 - val_accuracy: 0.5333\n",
            "Epoch 528/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.8638 - val_loss: 2.2552 - val_accuracy: 0.5233\n",
            "Epoch 529/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.8639 - val_loss: 2.2527 - val_accuracy: 0.5300\n",
            "Epoch 530/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4998 - accuracy: 0.8890 - val_loss: 2.2560 - val_accuracy: 0.5300\n",
            "Epoch 531/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.8615 - val_loss: 2.2573 - val_accuracy: 0.5300\n",
            "Epoch 532/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.8497 - val_loss: 2.2604 - val_accuracy: 0.5267\n",
            "Epoch 533/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8686 - val_loss: 2.2585 - val_accuracy: 0.5233\n",
            "Epoch 534/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.8840 - val_loss: 2.2715 - val_accuracy: 0.5333\n",
            "Epoch 535/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.8699 - val_loss: 2.2604 - val_accuracy: 0.5267\n",
            "Epoch 536/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.8749 - val_loss: 2.2519 - val_accuracy: 0.5233\n",
            "Epoch 537/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.8636 - val_loss: 2.2515 - val_accuracy: 0.5300\n",
            "Epoch 538/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8659 - val_loss: 2.2711 - val_accuracy: 0.5267\n",
            "Epoch 539/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.8674 - val_loss: 2.2561 - val_accuracy: 0.5233\n",
            "Epoch 540/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.8721 - val_loss: 2.2815 - val_accuracy: 0.5300\n",
            "Epoch 541/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.8684 - val_loss: 2.2651 - val_accuracy: 0.5233\n",
            "Epoch 542/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8833 - val_loss: 2.2713 - val_accuracy: 0.5200\n",
            "Epoch 543/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.8618 - val_loss: 2.2815 - val_accuracy: 0.5167\n",
            "Epoch 544/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.8418 - val_loss: 2.2810 - val_accuracy: 0.5233\n",
            "Epoch 545/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.8847 - val_loss: 2.2864 - val_accuracy: 0.5200\n",
            "Epoch 546/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.8373 - val_loss: 2.2891 - val_accuracy: 0.5267\n",
            "Epoch 547/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.8777 - val_loss: 2.2966 - val_accuracy: 0.5200\n",
            "Epoch 548/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.8715 - val_loss: 2.2813 - val_accuracy: 0.5167\n",
            "Epoch 549/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.8770 - val_loss: 2.2759 - val_accuracy: 0.5233\n",
            "Epoch 550/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.8668 - val_loss: 2.2848 - val_accuracy: 0.5133\n",
            "Epoch 551/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.8755 - val_loss: 2.2950 - val_accuracy: 0.5233\n",
            "Epoch 552/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.8702 - val_loss: 2.2838 - val_accuracy: 0.5200\n",
            "Epoch 553/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.8579 - val_loss: 2.2947 - val_accuracy: 0.5200\n",
            "Epoch 554/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.8560 - val_loss: 2.3035 - val_accuracy: 0.5167\n",
            "Epoch 555/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.8550 - val_loss: 2.2970 - val_accuracy: 0.5200\n",
            "Epoch 556/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.8858 - val_loss: 2.2963 - val_accuracy: 0.5167\n",
            "Epoch 557/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.8710 - val_loss: 2.3078 - val_accuracy: 0.5267\n",
            "Epoch 558/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8601 - val_loss: 2.3050 - val_accuracy: 0.5200\n",
            "Epoch 559/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.8801 - val_loss: 2.3035 - val_accuracy: 0.5267\n",
            "Epoch 560/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.8663 - val_loss: 2.3090 - val_accuracy: 0.5200\n",
            "Epoch 561/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8743 - val_loss: 2.3169 - val_accuracy: 0.5200\n",
            "Epoch 562/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.8500 - val_loss: 2.3137 - val_accuracy: 0.5200\n",
            "Epoch 563/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4816 - accuracy: 0.8663 - val_loss: 2.3263 - val_accuracy: 0.5267\n",
            "Epoch 564/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.8656 - val_loss: 2.3263 - val_accuracy: 0.5200\n",
            "Epoch 565/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.8681 - val_loss: 2.3348 - val_accuracy: 0.5200\n",
            "Epoch 566/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.8764 - val_loss: 2.3436 - val_accuracy: 0.5233\n",
            "Epoch 567/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8947 - val_loss: 2.3417 - val_accuracy: 0.5233\n",
            "Epoch 568/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.8783 - val_loss: 2.3322 - val_accuracy: 0.5167\n",
            "Epoch 569/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.8893 - val_loss: 2.3331 - val_accuracy: 0.5200\n",
            "Epoch 570/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.8876 - val_loss: 2.3429 - val_accuracy: 0.5200\n",
            "Epoch 571/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.8569 - val_loss: 2.3331 - val_accuracy: 0.5233\n",
            "Epoch 572/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.8731 - val_loss: 2.3530 - val_accuracy: 0.5167\n",
            "Epoch 573/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.8860 - val_loss: 2.3456 - val_accuracy: 0.5133\n",
            "Epoch 574/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.8784 - val_loss: 2.3257 - val_accuracy: 0.5200\n",
            "Epoch 575/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.8854 - val_loss: 2.3320 - val_accuracy: 0.5200\n",
            "Epoch 576/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.8962 - val_loss: 2.3623 - val_accuracy: 0.5133\n",
            "Epoch 577/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8951 - val_loss: 2.3501 - val_accuracy: 0.5133\n",
            "Epoch 578/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.8654 - val_loss: 2.3666 - val_accuracy: 0.5167\n",
            "Epoch 579/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.8733 - val_loss: 2.3613 - val_accuracy: 0.5167\n",
            "Epoch 580/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.8670 - val_loss: 2.3666 - val_accuracy: 0.5133\n",
            "Epoch 581/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.8754 - val_loss: 2.3726 - val_accuracy: 0.5133\n",
            "Epoch 582/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.8708 - val_loss: 2.3440 - val_accuracy: 0.5100\n",
            "Epoch 583/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.8695 - val_loss: 2.3517 - val_accuracy: 0.5200\n",
            "Epoch 584/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.8674 - val_loss: 2.3508 - val_accuracy: 0.5233\n",
            "Epoch 585/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.8763 - val_loss: 2.3639 - val_accuracy: 0.5233\n",
            "Epoch 586/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8783 - val_loss: 2.3783 - val_accuracy: 0.5167\n",
            "Epoch 587/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.8787 - val_loss: 2.3545 - val_accuracy: 0.5167\n",
            "Epoch 588/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.9039 - val_loss: 2.3739 - val_accuracy: 0.5167\n",
            "Epoch 589/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8724 - val_loss: 2.3754 - val_accuracy: 0.5167\n",
            "Epoch 590/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.8987 - val_loss: 2.3926 - val_accuracy: 0.5133\n",
            "Epoch 591/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.8686 - val_loss: 2.3756 - val_accuracy: 0.5200\n",
            "Epoch 592/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.8756 - val_loss: 2.3683 - val_accuracy: 0.5133\n",
            "Epoch 593/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.8622 - val_loss: 2.4020 - val_accuracy: 0.5133\n",
            "Epoch 594/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8778 - val_loss: 2.3706 - val_accuracy: 0.5167\n",
            "Epoch 595/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.8530 - val_loss: 2.3824 - val_accuracy: 0.5133\n",
            "Epoch 596/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.8782 - val_loss: 2.3926 - val_accuracy: 0.5167\n",
            "Epoch 597/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.8735 - val_loss: 2.3767 - val_accuracy: 0.5167\n",
            "Epoch 598/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.8963 - val_loss: 2.3940 - val_accuracy: 0.5233\n",
            "Epoch 599/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.8563 - val_loss: 2.4019 - val_accuracy: 0.5267\n",
            "Epoch 600/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.8791 - val_loss: 2.3849 - val_accuracy: 0.5200\n",
            "Epoch 601/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.8873 - val_loss: 2.3884 - val_accuracy: 0.5133\n",
            "Epoch 602/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.8892 - val_loss: 2.3884 - val_accuracy: 0.5200\n",
            "Epoch 603/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.8742 - val_loss: 2.3879 - val_accuracy: 0.5133\n",
            "Epoch 604/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.8837 - val_loss: 2.4163 - val_accuracy: 0.5100\n",
            "Epoch 605/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4774 - accuracy: 0.8743 - val_loss: 2.4020 - val_accuracy: 0.5167\n",
            "Epoch 606/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.8842 - val_loss: 2.4038 - val_accuracy: 0.5167\n",
            "Epoch 607/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.8836 - val_loss: 2.4137 - val_accuracy: 0.5167\n",
            "Epoch 608/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.8995 - val_loss: 2.4276 - val_accuracy: 0.5233\n",
            "Epoch 609/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.8837 - val_loss: 2.4191 - val_accuracy: 0.5167\n",
            "Epoch 610/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.8800 - val_loss: 2.4207 - val_accuracy: 0.5133\n",
            "Epoch 611/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.8994 - val_loss: 2.4164 - val_accuracy: 0.5167\n",
            "Epoch 612/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.8998 - val_loss: 2.4125 - val_accuracy: 0.5100\n",
            "Epoch 613/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8902 - val_loss: 2.4240 - val_accuracy: 0.5133\n",
            "Epoch 614/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.8877 - val_loss: 2.4169 - val_accuracy: 0.5200\n",
            "Epoch 615/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.9039 - val_loss: 2.4151 - val_accuracy: 0.5033\n",
            "Epoch 616/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8897 - val_loss: 2.4159 - val_accuracy: 0.5133\n",
            "Epoch 617/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8684 - val_loss: 2.4435 - val_accuracy: 0.5167\n",
            "Epoch 618/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.8730 - val_loss: 2.4194 - val_accuracy: 0.5133\n",
            "Epoch 619/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.8715 - val_loss: 2.4309 - val_accuracy: 0.5133\n",
            "Epoch 620/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.8954 - val_loss: 2.4482 - val_accuracy: 0.5100\n",
            "Epoch 621/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.9017 - val_loss: 2.4307 - val_accuracy: 0.5167\n",
            "Epoch 622/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.8823 - val_loss: 2.4442 - val_accuracy: 0.5133\n",
            "Epoch 623/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.8844 - val_loss: 2.4380 - val_accuracy: 0.5100\n",
            "Epoch 624/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.8842 - val_loss: 2.4442 - val_accuracy: 0.5133\n",
            "Epoch 625/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.8909 - val_loss: 2.4386 - val_accuracy: 0.5033\n",
            "Epoch 626/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.8886 - val_loss: 2.4543 - val_accuracy: 0.5100\n",
            "Epoch 627/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.8738 - val_loss: 2.4477 - val_accuracy: 0.5100\n",
            "Epoch 628/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4504 - accuracy: 0.8910 - val_loss: 2.4328 - val_accuracy: 0.5100\n",
            "Epoch 629/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4576 - accuracy: 0.8936 - val_loss: 2.4576 - val_accuracy: 0.5133\n",
            "Epoch 630/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.8901 - val_loss: 2.4501 - val_accuracy: 0.5167\n",
            "Epoch 631/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4580 - accuracy: 0.8808 - val_loss: 2.4555 - val_accuracy: 0.5133\n",
            "Epoch 632/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.9106 - val_loss: 2.4585 - val_accuracy: 0.5100\n",
            "Epoch 633/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.8830 - val_loss: 2.4740 - val_accuracy: 0.5067\n",
            "Epoch 634/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.8862 - val_loss: 2.4584 - val_accuracy: 0.5200\n",
            "Epoch 635/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.9010 - val_loss: 2.4776 - val_accuracy: 0.5133\n",
            "Epoch 636/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.9062 - val_loss: 2.4658 - val_accuracy: 0.5067\n",
            "Epoch 637/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.8963 - val_loss: 2.4833 - val_accuracy: 0.5100\n",
            "Epoch 638/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.8650 - val_loss: 2.4652 - val_accuracy: 0.5133\n",
            "Epoch 639/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8924 - val_loss: 2.4761 - val_accuracy: 0.5200\n",
            "Epoch 640/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.8918 - val_loss: 2.4905 - val_accuracy: 0.5067\n",
            "Epoch 641/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8802 - val_loss: 2.4713 - val_accuracy: 0.5100\n",
            "Epoch 642/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8935 - val_loss: 2.4811 - val_accuracy: 0.5167\n",
            "Epoch 643/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.8900 - val_loss: 2.4799 - val_accuracy: 0.5067\n",
            "Epoch 644/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.8844 - val_loss: 2.4921 - val_accuracy: 0.5000\n",
            "Epoch 645/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8967 - val_loss: 2.4879 - val_accuracy: 0.5067\n",
            "Epoch 646/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8903 - val_loss: 2.5115 - val_accuracy: 0.5167\n",
            "Epoch 647/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.8973 - val_loss: 2.5023 - val_accuracy: 0.5067\n",
            "Epoch 648/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8758 - val_loss: 2.4906 - val_accuracy: 0.5067\n",
            "Epoch 649/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.8815 - val_loss: 2.4998 - val_accuracy: 0.5100\n",
            "Epoch 650/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8958 - val_loss: 2.4985 - val_accuracy: 0.5067\n",
            "Epoch 651/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.9091 - val_loss: 2.4952 - val_accuracy: 0.5000\n",
            "Epoch 652/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.8984 - val_loss: 2.4990 - val_accuracy: 0.5067\n",
            "Epoch 653/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.9105 - val_loss: 2.5382 - val_accuracy: 0.5100\n",
            "Epoch 654/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.9037 - val_loss: 2.5104 - val_accuracy: 0.5200\n",
            "Epoch 655/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.9048 - val_loss: 2.5229 - val_accuracy: 0.5067\n",
            "Epoch 656/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8966 - val_loss: 2.5166 - val_accuracy: 0.5033\n",
            "Epoch 657/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.5367 - accuracy: 0.8899 - val_loss: 2.5131 - val_accuracy: 0.5067\n",
            "Epoch 658/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.8904 - val_loss: 2.5324 - val_accuracy: 0.5033\n",
            "Epoch 659/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.8740 - val_loss: 2.5105 - val_accuracy: 0.5100\n",
            "Epoch 660/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.8726 - val_loss: 2.5298 - val_accuracy: 0.5100\n",
            "Epoch 661/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.8923 - val_loss: 2.5366 - val_accuracy: 0.5033\n",
            "Epoch 662/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8942 - val_loss: 2.5322 - val_accuracy: 0.5233\n",
            "Epoch 663/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.8679 - val_loss: 2.5314 - val_accuracy: 0.5033\n",
            "Epoch 664/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.8813 - val_loss: 2.5428 - val_accuracy: 0.5100\n",
            "Epoch 665/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.9141 - val_loss: 2.5314 - val_accuracy: 0.5000\n",
            "Epoch 666/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.8962 - val_loss: 2.5574 - val_accuracy: 0.5067\n",
            "Epoch 667/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8777 - val_loss: 2.5447 - val_accuracy: 0.5033\n",
            "Epoch 668/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.8898 - val_loss: 2.5380 - val_accuracy: 0.5100\n",
            "Epoch 669/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8964 - val_loss: 2.5496 - val_accuracy: 0.5100\n",
            "Epoch 670/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.8936 - val_loss: 2.5557 - val_accuracy: 0.5033\n",
            "Epoch 671/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8962 - val_loss: 2.5363 - val_accuracy: 0.5067\n",
            "Epoch 672/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8786 - val_loss: 2.5427 - val_accuracy: 0.5200\n",
            "Epoch 673/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.8690 - val_loss: 2.5703 - val_accuracy: 0.5067\n",
            "Epoch 674/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.8988 - val_loss: 2.5636 - val_accuracy: 0.5000\n",
            "Epoch 675/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.8812 - val_loss: 2.5425 - val_accuracy: 0.5067\n",
            "Epoch 676/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8791 - val_loss: 2.5415 - val_accuracy: 0.5133\n",
            "Epoch 677/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.8895 - val_loss: 2.5569 - val_accuracy: 0.5033\n",
            "Epoch 678/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8754 - val_loss: 2.5543 - val_accuracy: 0.5000\n",
            "Epoch 679/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.9003 - val_loss: 2.5669 - val_accuracy: 0.5200\n",
            "Epoch 680/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.8955 - val_loss: 2.5731 - val_accuracy: 0.5133\n",
            "Epoch 681/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.8896 - val_loss: 2.5955 - val_accuracy: 0.5167\n",
            "Epoch 682/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8791 - val_loss: 2.5753 - val_accuracy: 0.5033\n",
            "Epoch 683/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4160 - accuracy: 0.9058 - val_loss: 2.5595 - val_accuracy: 0.5100\n",
            "Epoch 684/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.9040 - val_loss: 2.5687 - val_accuracy: 0.5100\n",
            "Epoch 685/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.8853 - val_loss: 2.5865 - val_accuracy: 0.5067\n",
            "Epoch 686/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.9289 - val_loss: 2.5695 - val_accuracy: 0.5100\n",
            "Epoch 687/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8858 - val_loss: 2.5789 - val_accuracy: 0.5000\n",
            "Epoch 688/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.9149 - val_loss: 2.6014 - val_accuracy: 0.4967\n",
            "Epoch 689/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.8967 - val_loss: 2.5966 - val_accuracy: 0.5067\n",
            "Epoch 690/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.9164 - val_loss: 2.5992 - val_accuracy: 0.5033\n",
            "Epoch 691/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.8771 - val_loss: 2.6139 - val_accuracy: 0.5200\n",
            "Epoch 692/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.9160 - val_loss: 2.5992 - val_accuracy: 0.5133\n",
            "Epoch 693/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8870 - val_loss: 2.5811 - val_accuracy: 0.5067\n",
            "Epoch 694/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.8931 - val_loss: 2.6019 - val_accuracy: 0.5200\n",
            "Epoch 695/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.9029 - val_loss: 2.5969 - val_accuracy: 0.5167\n",
            "Epoch 696/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.8756 - val_loss: 2.6081 - val_accuracy: 0.5100\n",
            "Epoch 697/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8969 - val_loss: 2.5999 - val_accuracy: 0.5067\n",
            "Epoch 698/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8967 - val_loss: 2.6007 - val_accuracy: 0.5067\n",
            "Epoch 699/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.9032 - val_loss: 2.6298 - val_accuracy: 0.5033\n",
            "Epoch 700/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4305 - accuracy: 0.8967 - val_loss: 2.6311 - val_accuracy: 0.4933\n",
            "Epoch 701/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8904 - val_loss: 2.6313 - val_accuracy: 0.5133\n",
            "Epoch 702/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.8851 - val_loss: 2.6351 - val_accuracy: 0.4967\n",
            "Epoch 703/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.9096 - val_loss: 2.6311 - val_accuracy: 0.5067\n",
            "Epoch 704/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.9122 - val_loss: 2.6185 - val_accuracy: 0.5133\n",
            "Epoch 705/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.8806 - val_loss: 2.6269 - val_accuracy: 0.5033\n",
            "Epoch 706/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.8894 - val_loss: 2.6158 - val_accuracy: 0.5033\n",
            "Epoch 707/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8945 - val_loss: 2.6297 - val_accuracy: 0.5067\n",
            "Epoch 708/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.9043 - val_loss: 2.6578 - val_accuracy: 0.4933\n",
            "Epoch 709/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.9000 - val_loss: 2.6338 - val_accuracy: 0.5133\n",
            "Epoch 710/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.4083 - accuracy: 0.9068 - val_loss: 2.6463 - val_accuracy: 0.5267\n",
            "Epoch 711/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.9011 - val_loss: 2.6348 - val_accuracy: 0.5033\n",
            "Epoch 712/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.9026 - val_loss: 2.6459 - val_accuracy: 0.5000\n",
            "Epoch 713/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.9009 - val_loss: 2.6170 - val_accuracy: 0.5200\n",
            "Epoch 714/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8982 - val_loss: 2.6332 - val_accuracy: 0.5233\n",
            "Epoch 715/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.9007 - val_loss: 2.6465 - val_accuracy: 0.5133\n",
            "Epoch 716/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.9089 - val_loss: 2.6466 - val_accuracy: 0.5100\n",
            "Epoch 717/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.9091 - val_loss: 2.6504 - val_accuracy: 0.5033\n",
            "Epoch 718/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.9128 - val_loss: 2.6568 - val_accuracy: 0.5200\n",
            "Epoch 719/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.9038 - val_loss: 2.6638 - val_accuracy: 0.4967\n",
            "Epoch 720/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.9132 - val_loss: 2.6483 - val_accuracy: 0.5133\n",
            "Epoch 721/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.9112 - val_loss: 2.6702 - val_accuracy: 0.4933\n",
            "Epoch 722/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.9146 - val_loss: 2.6663 - val_accuracy: 0.5100\n",
            "Epoch 723/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.9015 - val_loss: 2.6702 - val_accuracy: 0.5167\n",
            "Epoch 724/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8985 - val_loss: 2.6869 - val_accuracy: 0.5000\n",
            "Epoch 725/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.8949 - val_loss: 2.6911 - val_accuracy: 0.5033\n",
            "Epoch 726/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.8943 - val_loss: 2.6759 - val_accuracy: 0.4933\n",
            "Epoch 727/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.9084 - val_loss: 2.6693 - val_accuracy: 0.4933\n",
            "Epoch 728/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.9082 - val_loss: 2.6718 - val_accuracy: 0.4967\n",
            "Epoch 729/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.8817 - val_loss: 2.6826 - val_accuracy: 0.5167\n",
            "Epoch 730/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.9094 - val_loss: 2.6922 - val_accuracy: 0.5133\n",
            "Epoch 731/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8921 - val_loss: 2.6713 - val_accuracy: 0.5233\n",
            "Epoch 732/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8887 - val_loss: 2.6859 - val_accuracy: 0.5100\n",
            "Epoch 733/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.9136 - val_loss: 2.6983 - val_accuracy: 0.5000\n",
            "Epoch 734/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.9125 - val_loss: 2.7146 - val_accuracy: 0.5100\n",
            "Epoch 735/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.9100 - val_loss: 2.6825 - val_accuracy: 0.5200\n",
            "Epoch 736/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.9297 - val_loss: 2.6690 - val_accuracy: 0.5233\n",
            "Epoch 737/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.4181 - accuracy: 0.8963 - val_loss: 2.6854 - val_accuracy: 0.5200\n",
            "Epoch 738/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4220 - accuracy: 0.8923 - val_loss: 2.7109 - val_accuracy: 0.4833\n",
            "Epoch 739/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.9080 - val_loss: 2.7043 - val_accuracy: 0.5133\n",
            "Epoch 740/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8950 - val_loss: 2.7109 - val_accuracy: 0.5133\n",
            "Epoch 741/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.9069 - val_loss: 2.7014 - val_accuracy: 0.5067\n",
            "Epoch 742/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.8810 - val_loss: 2.7123 - val_accuracy: 0.4900\n",
            "Epoch 743/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.9035 - val_loss: 2.7113 - val_accuracy: 0.5133\n",
            "Epoch 744/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.9152 - val_loss: 2.7284 - val_accuracy: 0.5100\n",
            "Epoch 745/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.8939 - val_loss: 2.7129 - val_accuracy: 0.5067\n",
            "Epoch 746/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.9022 - val_loss: 2.6983 - val_accuracy: 0.5167\n",
            "Epoch 747/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.9041 - val_loss: 2.7250 - val_accuracy: 0.4900\n",
            "Epoch 748/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.9238 - val_loss: 2.7195 - val_accuracy: 0.4933\n",
            "Epoch 749/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.9016 - val_loss: 2.7341 - val_accuracy: 0.5033\n",
            "Epoch 750/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.9037 - val_loss: 2.7110 - val_accuracy: 0.5100\n",
            "Epoch 751/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.9030 - val_loss: 2.7252 - val_accuracy: 0.5033\n",
            "Epoch 752/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4130 - accuracy: 0.8936 - val_loss: 2.7266 - val_accuracy: 0.4967\n",
            "Epoch 753/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4058 - accuracy: 0.9050 - val_loss: 2.7415 - val_accuracy: 0.5067\n",
            "Epoch 754/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4467 - accuracy: 0.9070 - val_loss: 2.7195 - val_accuracy: 0.5100\n",
            "Epoch 755/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4218 - accuracy: 0.8989 - val_loss: 2.7184 - val_accuracy: 0.5133\n",
            "Epoch 756/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.9191 - val_loss: 2.7205 - val_accuracy: 0.5100\n",
            "Epoch 757/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.9052 - val_loss: 2.7440 - val_accuracy: 0.5067\n",
            "Epoch 758/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.9067 - val_loss: 2.7594 - val_accuracy: 0.5167\n",
            "Epoch 759/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4193 - accuracy: 0.8898 - val_loss: 2.7601 - val_accuracy: 0.4900\n",
            "Epoch 760/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.9049 - val_loss: 2.7315 - val_accuracy: 0.5067\n",
            "Epoch 761/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.9113 - val_loss: 2.7467 - val_accuracy: 0.5100\n",
            "Epoch 762/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.9006 - val_loss: 2.7448 - val_accuracy: 0.5133\n",
            "Epoch 763/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.9093 - val_loss: 2.7577 - val_accuracy: 0.4900\n",
            "Epoch 764/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.9120 - val_loss: 2.7420 - val_accuracy: 0.5100\n",
            "Epoch 765/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.8964 - val_loss: 2.7621 - val_accuracy: 0.5100\n",
            "Epoch 766/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.9091 - val_loss: 2.7616 - val_accuracy: 0.5100\n",
            "Epoch 767/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.9001 - val_loss: 2.7554 - val_accuracy: 0.5033\n",
            "Epoch 768/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.9114 - val_loss: 2.7700 - val_accuracy: 0.5033\n",
            "Epoch 769/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.9016 - val_loss: 2.7709 - val_accuracy: 0.5000\n",
            "Epoch 770/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.9182 - val_loss: 2.7649 - val_accuracy: 0.5133\n",
            "Epoch 771/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.9041 - val_loss: 2.7597 - val_accuracy: 0.5100\n",
            "Epoch 772/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.9135 - val_loss: 2.7712 - val_accuracy: 0.5133\n",
            "Epoch 773/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8944 - val_loss: 2.7874 - val_accuracy: 0.4933\n",
            "Epoch 774/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.9157 - val_loss: 2.7785 - val_accuracy: 0.4967\n",
            "Epoch 775/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.9052 - val_loss: 2.7782 - val_accuracy: 0.4867\n",
            "Epoch 776/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8967 - val_loss: 2.7882 - val_accuracy: 0.5000\n",
            "Epoch 777/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.9101 - val_loss: 2.7723 - val_accuracy: 0.5033\n",
            "Epoch 778/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.9071 - val_loss: 2.7693 - val_accuracy: 0.5133\n",
            "Epoch 779/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.9139 - val_loss: 2.7817 - val_accuracy: 0.5067\n",
            "Epoch 780/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.9038 - val_loss: 2.7985 - val_accuracy: 0.4900\n",
            "Epoch 781/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.9125 - val_loss: 2.7881 - val_accuracy: 0.5033\n",
            "Epoch 782/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.9058 - val_loss: 2.7761 - val_accuracy: 0.5033\n",
            "Epoch 783/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8992 - val_loss: 2.7872 - val_accuracy: 0.4967\n",
            "Epoch 784/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8921 - val_loss: 2.8007 - val_accuracy: 0.4967\n",
            "Epoch 785/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.8968 - val_loss: 2.7994 - val_accuracy: 0.5033\n",
            "Epoch 786/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.9047 - val_loss: 2.7907 - val_accuracy: 0.5033\n",
            "Epoch 787/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.9245 - val_loss: 2.7894 - val_accuracy: 0.5033\n",
            "Epoch 788/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.9028 - val_loss: 2.8047 - val_accuracy: 0.5100\n",
            "Epoch 789/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.9202 - val_loss: 2.8051 - val_accuracy: 0.4967\n",
            "Epoch 790/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.9124 - val_loss: 2.8223 - val_accuracy: 0.4867\n",
            "Epoch 791/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.9037 - val_loss: 2.8115 - val_accuracy: 0.4967\n",
            "Epoch 792/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.9079 - val_loss: 2.8166 - val_accuracy: 0.4967\n",
            "Epoch 793/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.9157 - val_loss: 2.8289 - val_accuracy: 0.4967\n",
            "Epoch 794/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.9138 - val_loss: 2.7968 - val_accuracy: 0.5133\n",
            "Epoch 795/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.9103 - val_loss: 2.8195 - val_accuracy: 0.5000\n",
            "Epoch 796/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.9169 - val_loss: 2.8059 - val_accuracy: 0.5033\n",
            "Epoch 797/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.9011 - val_loss: 2.8215 - val_accuracy: 0.4967\n",
            "Epoch 798/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.9146 - val_loss: 2.8206 - val_accuracy: 0.5033\n",
            "Epoch 799/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.9302 - val_loss: 2.8276 - val_accuracy: 0.4967\n",
            "Epoch 800/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8991 - val_loss: 2.8205 - val_accuracy: 0.5000\n",
            "Epoch 801/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.9246 - val_loss: 2.8216 - val_accuracy: 0.5067\n",
            "Epoch 802/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.9143 - val_loss: 2.8419 - val_accuracy: 0.5000\n",
            "Epoch 803/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.9138 - val_loss: 2.8336 - val_accuracy: 0.5067\n",
            "Epoch 804/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.9198 - val_loss: 2.8290 - val_accuracy: 0.5033\n",
            "Epoch 805/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.9002 - val_loss: 2.8364 - val_accuracy: 0.5033\n",
            "Epoch 806/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.9179 - val_loss: 2.8495 - val_accuracy: 0.4900\n",
            "Epoch 807/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.9182 - val_loss: 2.8266 - val_accuracy: 0.5000\n",
            "Epoch 808/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.9283 - val_loss: 2.8469 - val_accuracy: 0.5033\n",
            "Epoch 809/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4193 - accuracy: 0.8940 - val_loss: 2.8464 - val_accuracy: 0.4900\n",
            "Epoch 810/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3635 - accuracy: 0.9217 - val_loss: 2.8526 - val_accuracy: 0.5000\n",
            "Epoch 811/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.9095 - val_loss: 2.8372 - val_accuracy: 0.5033\n",
            "Epoch 812/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.9139 - val_loss: 2.8593 - val_accuracy: 0.5000\n",
            "Epoch 813/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.9066 - val_loss: 2.8618 - val_accuracy: 0.4967\n",
            "Epoch 814/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.9140 - val_loss: 2.8539 - val_accuracy: 0.5000\n",
            "Epoch 815/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.9100 - val_loss: 2.8471 - val_accuracy: 0.5067\n",
            "Epoch 816/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.9138 - val_loss: 2.8619 - val_accuracy: 0.4967\n",
            "Epoch 817/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.9252 - val_loss: 2.8539 - val_accuracy: 0.4967\n",
            "Epoch 818/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.8898 - val_loss: 2.8745 - val_accuracy: 0.4967\n",
            "Epoch 819/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.9215 - val_loss: 2.8758 - val_accuracy: 0.5033\n",
            "Epoch 820/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.9208 - val_loss: 2.8728 - val_accuracy: 0.4933\n",
            "Epoch 821/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.9080 - val_loss: 2.8593 - val_accuracy: 0.5000\n",
            "Epoch 822/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.9105 - val_loss: 2.8636 - val_accuracy: 0.5033\n",
            "Epoch 823/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.9128 - val_loss: 2.8781 - val_accuracy: 0.4933\n",
            "Epoch 824/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.9204 - val_loss: 2.8663 - val_accuracy: 0.5033\n",
            "Epoch 825/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.9226 - val_loss: 2.8875 - val_accuracy: 0.5000\n",
            "Epoch 826/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.9083 - val_loss: 2.8961 - val_accuracy: 0.5033\n",
            "Epoch 827/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.9400 - val_loss: 2.8878 - val_accuracy: 0.4967\n",
            "Epoch 828/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.9163 - val_loss: 2.8750 - val_accuracy: 0.5067\n",
            "Epoch 829/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4058 - accuracy: 0.9118 - val_loss: 2.8966 - val_accuracy: 0.5000\n",
            "Epoch 830/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.9211 - val_loss: 2.8955 - val_accuracy: 0.5067\n",
            "Epoch 831/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.9135 - val_loss: 2.8933 - val_accuracy: 0.4933\n",
            "Epoch 832/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8901 - val_loss: 2.8873 - val_accuracy: 0.5067\n",
            "Epoch 833/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.9287 - val_loss: 2.8919 - val_accuracy: 0.5033\n",
            "Epoch 834/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.9126 - val_loss: 2.9091 - val_accuracy: 0.5000\n",
            "Epoch 835/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.9213 - val_loss: 2.8951 - val_accuracy: 0.5067\n",
            "Epoch 836/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.9187 - val_loss: 2.8980 - val_accuracy: 0.5033\n",
            "Epoch 837/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.9199 - val_loss: 2.8934 - val_accuracy: 0.5000\n",
            "Epoch 838/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.9105 - val_loss: 2.9041 - val_accuracy: 0.5033\n",
            "Epoch 839/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.9270 - val_loss: 2.9039 - val_accuracy: 0.5067\n",
            "Epoch 840/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.9201 - val_loss: 2.9075 - val_accuracy: 0.5033\n",
            "Epoch 841/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.9023 - val_loss: 2.9128 - val_accuracy: 0.5067\n",
            "Epoch 842/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.9087 - val_loss: 2.9155 - val_accuracy: 0.5000\n",
            "Epoch 843/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.9283 - val_loss: 2.9270 - val_accuracy: 0.4933\n",
            "Epoch 844/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.9177 - val_loss: 2.9308 - val_accuracy: 0.4933\n",
            "Epoch 845/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.9315 - val_loss: 2.9167 - val_accuracy: 0.5033\n",
            "Epoch 846/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.9213 - val_loss: 2.9394 - val_accuracy: 0.5067\n",
            "Epoch 847/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3496 - accuracy: 0.9248 - val_loss: 2.9099 - val_accuracy: 0.5067\n",
            "Epoch 848/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.9146 - val_loss: 2.9169 - val_accuracy: 0.5033\n",
            "Epoch 849/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.9230 - val_loss: 2.9260 - val_accuracy: 0.4967\n",
            "Epoch 850/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.9199 - val_loss: 2.9072 - val_accuracy: 0.5000\n",
            "Epoch 851/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.9248 - val_loss: 2.9154 - val_accuracy: 0.4967\n",
            "Epoch 852/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.9127 - val_loss: 2.9273 - val_accuracy: 0.5033\n",
            "Epoch 853/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.9124 - val_loss: 2.9380 - val_accuracy: 0.5033\n",
            "Epoch 854/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.9104 - val_loss: 2.9349 - val_accuracy: 0.5033\n",
            "Epoch 855/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.9065 - val_loss: 2.9345 - val_accuracy: 0.5033\n",
            "Epoch 856/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.9169 - val_loss: 2.9405 - val_accuracy: 0.4933\n",
            "Epoch 857/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.9131 - val_loss: 2.9452 - val_accuracy: 0.4967\n",
            "Epoch 858/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.9124 - val_loss: 2.9263 - val_accuracy: 0.5000\n",
            "Epoch 859/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8994 - val_loss: 2.9594 - val_accuracy: 0.5000\n",
            "Epoch 860/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.9141 - val_loss: 2.9543 - val_accuracy: 0.5000\n",
            "Epoch 861/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.9319 - val_loss: 2.9430 - val_accuracy: 0.5000\n",
            "Epoch 862/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.9254 - val_loss: 2.9546 - val_accuracy: 0.5067\n",
            "Epoch 863/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.9120 - val_loss: 2.9452 - val_accuracy: 0.5000\n",
            "Epoch 864/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.8919 - val_loss: 2.9846 - val_accuracy: 0.4933\n",
            "Epoch 865/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.9039 - val_loss: 2.9652 - val_accuracy: 0.5100\n",
            "Epoch 866/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4229 - accuracy: 0.9007 - val_loss: 2.9643 - val_accuracy: 0.5000\n",
            "Epoch 867/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.9204 - val_loss: 2.9459 - val_accuracy: 0.5000\n",
            "Epoch 868/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.9087 - val_loss: 2.9696 - val_accuracy: 0.5000\n",
            "Epoch 869/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.9352 - val_loss: 2.9745 - val_accuracy: 0.5000\n",
            "Epoch 870/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.9124 - val_loss: 2.9812 - val_accuracy: 0.4900\n",
            "Epoch 871/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.9347 - val_loss: 2.9595 - val_accuracy: 0.5100\n",
            "Epoch 872/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.9288 - val_loss: 2.9694 - val_accuracy: 0.5000\n",
            "Epoch 873/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.9084 - val_loss: 2.9903 - val_accuracy: 0.5033\n",
            "Epoch 874/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.9204 - val_loss: 2.9634 - val_accuracy: 0.5033\n",
            "Epoch 875/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.9070 - val_loss: 2.9818 - val_accuracy: 0.5000\n",
            "Epoch 876/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.9123 - val_loss: 2.9809 - val_accuracy: 0.5067\n",
            "Epoch 877/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.9037 - val_loss: 2.9863 - val_accuracy: 0.4967\n",
            "Epoch 878/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.9093 - val_loss: 2.9808 - val_accuracy: 0.4967\n",
            "Epoch 879/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.9246 - val_loss: 2.9746 - val_accuracy: 0.5000\n",
            "Epoch 880/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.9300 - val_loss: 2.9736 - val_accuracy: 0.4967\n",
            "Epoch 881/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.9177 - val_loss: 3.0056 - val_accuracy: 0.4933\n",
            "Epoch 882/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.9187 - val_loss: 2.9839 - val_accuracy: 0.5000\n",
            "Epoch 883/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.4243 - accuracy: 0.9173 - val_loss: 3.0040 - val_accuracy: 0.4967\n",
            "Epoch 884/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.9079 - val_loss: 2.9963 - val_accuracy: 0.5000\n",
            "Epoch 885/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.9171 - val_loss: 2.9766 - val_accuracy: 0.5033\n",
            "Epoch 886/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.9062 - val_loss: 2.9972 - val_accuracy: 0.5033\n",
            "Epoch 887/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.9229 - val_loss: 3.0073 - val_accuracy: 0.5067\n",
            "Epoch 888/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.9136 - val_loss: 2.9877 - val_accuracy: 0.4967\n",
            "Epoch 889/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.9237 - val_loss: 2.9861 - val_accuracy: 0.4967\n",
            "Epoch 890/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.4016 - accuracy: 0.9135 - val_loss: 2.9750 - val_accuracy: 0.5000\n",
            "Epoch 891/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.9208 - val_loss: 2.9914 - val_accuracy: 0.5000\n",
            "Epoch 892/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.9202 - val_loss: 2.9910 - val_accuracy: 0.5000\n",
            "Epoch 893/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.9030 - val_loss: 2.9996 - val_accuracy: 0.4967\n",
            "Epoch 894/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.9151 - val_loss: 2.9915 - val_accuracy: 0.5000\n",
            "Epoch 895/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.9022 - val_loss: 3.0186 - val_accuracy: 0.5000\n",
            "Epoch 896/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.9179 - val_loss: 3.0071 - val_accuracy: 0.5000\n",
            "Epoch 897/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.9234 - val_loss: 3.0219 - val_accuracy: 0.4967\n",
            "Epoch 898/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.9189 - val_loss: 3.0064 - val_accuracy: 0.4933\n",
            "Epoch 899/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.9255 - val_loss: 3.0112 - val_accuracy: 0.5033\n",
            "Epoch 900/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.9181 - val_loss: 3.0348 - val_accuracy: 0.4967\n",
            "Epoch 901/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.9056 - val_loss: 3.0311 - val_accuracy: 0.4967\n",
            "Epoch 902/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.9063 - val_loss: 3.0142 - val_accuracy: 0.5033\n",
            "Epoch 903/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.9261 - val_loss: 3.0349 - val_accuracy: 0.5000\n",
            "Epoch 904/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.9182 - val_loss: 3.0256 - val_accuracy: 0.5000\n",
            "Epoch 905/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.9228 - val_loss: 3.0449 - val_accuracy: 0.5000\n",
            "Epoch 906/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.9294 - val_loss: 3.0409 - val_accuracy: 0.5000\n",
            "Epoch 907/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.9110 - val_loss: 3.0417 - val_accuracy: 0.4967\n",
            "Epoch 908/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.9094 - val_loss: 3.0580 - val_accuracy: 0.4900\n",
            "Epoch 909/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.9351 - val_loss: 3.0532 - val_accuracy: 0.4967\n",
            "Epoch 910/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3627 - accuracy: 0.9164 - val_loss: 3.0640 - val_accuracy: 0.5000\n",
            "Epoch 911/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.9332 - val_loss: 3.0519 - val_accuracy: 0.5033\n",
            "Epoch 912/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.9119 - val_loss: 3.0566 - val_accuracy: 0.5033\n",
            "Epoch 913/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.9217 - val_loss: 3.0453 - val_accuracy: 0.4967\n",
            "Epoch 914/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.9151 - val_loss: 3.0541 - val_accuracy: 0.5000\n",
            "Epoch 915/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.9295 - val_loss: 3.0661 - val_accuracy: 0.4933\n",
            "Epoch 916/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.9191 - val_loss: 3.0668 - val_accuracy: 0.4967\n",
            "Epoch 917/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.9244 - val_loss: 3.0541 - val_accuracy: 0.4967\n",
            "Epoch 918/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.9187 - val_loss: 3.0634 - val_accuracy: 0.5000\n",
            "Epoch 919/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.9179 - val_loss: 3.0727 - val_accuracy: 0.4967\n",
            "Epoch 920/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.9121 - val_loss: 3.0771 - val_accuracy: 0.5000\n",
            "Epoch 921/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3739 - accuracy: 0.9268 - val_loss: 3.0434 - val_accuracy: 0.5000\n",
            "Epoch 922/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.9129 - val_loss: 3.0743 - val_accuracy: 0.5000\n",
            "Epoch 923/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8919 - val_loss: 3.0438 - val_accuracy: 0.5033\n",
            "Epoch 924/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.9203 - val_loss: 3.0683 - val_accuracy: 0.5000\n",
            "Epoch 925/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.9191 - val_loss: 3.0574 - val_accuracy: 0.5000\n",
            "Epoch 926/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.9217 - val_loss: 3.0947 - val_accuracy: 0.4900\n",
            "Epoch 927/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.9092 - val_loss: 3.0761 - val_accuracy: 0.5000\n",
            "Epoch 928/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.9258 - val_loss: 3.0656 - val_accuracy: 0.5000\n",
            "Epoch 929/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.9100 - val_loss: 3.0705 - val_accuracy: 0.4967\n",
            "Epoch 930/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.9025 - val_loss: 3.0848 - val_accuracy: 0.4967\n",
            "Epoch 931/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.9249 - val_loss: 3.0708 - val_accuracy: 0.5000\n",
            "Epoch 932/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.9247 - val_loss: 3.0864 - val_accuracy: 0.5000\n",
            "Epoch 933/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.9051 - val_loss: 3.0898 - val_accuracy: 0.5000\n",
            "Epoch 934/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.9234 - val_loss: 3.0713 - val_accuracy: 0.5000\n",
            "Epoch 935/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.9295 - val_loss: 3.0808 - val_accuracy: 0.4933\n",
            "Epoch 936/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.9160 - val_loss: 3.1172 - val_accuracy: 0.4967\n",
            "Epoch 937/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.9397 - val_loss: 3.0897 - val_accuracy: 0.4967\n",
            "Epoch 938/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.9379 - val_loss: 3.1116 - val_accuracy: 0.5000\n",
            "Epoch 939/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.9277 - val_loss: 3.0800 - val_accuracy: 0.5000\n",
            "Epoch 940/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.9226 - val_loss: 3.0916 - val_accuracy: 0.5033\n",
            "Epoch 941/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.9142 - val_loss: 3.0731 - val_accuracy: 0.5000\n",
            "Epoch 942/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.9218 - val_loss: 3.0958 - val_accuracy: 0.4967\n",
            "Epoch 943/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.9279 - val_loss: 3.1324 - val_accuracy: 0.5000\n",
            "Epoch 944/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.9296 - val_loss: 3.1123 - val_accuracy: 0.5000\n",
            "Epoch 945/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.9120 - val_loss: 3.0961 - val_accuracy: 0.5033\n",
            "Epoch 946/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.9365 - val_loss: 3.1218 - val_accuracy: 0.5000\n",
            "Epoch 947/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.9339 - val_loss: 3.1133 - val_accuracy: 0.4967\n",
            "Epoch 948/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.9128 - val_loss: 3.1039 - val_accuracy: 0.5000\n",
            "Epoch 949/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.9273 - val_loss: 3.1294 - val_accuracy: 0.5000\n",
            "Epoch 950/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.9253 - val_loss: 3.1198 - val_accuracy: 0.4967\n",
            "Epoch 951/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.9240 - val_loss: 3.1227 - val_accuracy: 0.4933\n",
            "Epoch 952/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.9190 - val_loss: 3.1337 - val_accuracy: 0.4967\n",
            "Epoch 953/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.9078 - val_loss: 3.1358 - val_accuracy: 0.4933\n",
            "Epoch 954/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.9250 - val_loss: 3.1444 - val_accuracy: 0.5000\n",
            "Epoch 955/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.9178 - val_loss: 3.1099 - val_accuracy: 0.5033\n",
            "Epoch 956/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.9244 - val_loss: 3.1314 - val_accuracy: 0.4967\n",
            "Epoch 957/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.9225 - val_loss: 3.1252 - val_accuracy: 0.4967\n",
            "Epoch 958/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.9186 - val_loss: 3.1192 - val_accuracy: 0.5000\n",
            "Epoch 959/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.9312 - val_loss: 3.1445 - val_accuracy: 0.4900\n",
            "Epoch 960/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.9071 - val_loss: 3.1336 - val_accuracy: 0.4933\n",
            "Epoch 961/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.9291 - val_loss: 3.1443 - val_accuracy: 0.5033\n",
            "Epoch 962/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.9198 - val_loss: 3.1388 - val_accuracy: 0.5000\n",
            "Epoch 963/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.9033 - val_loss: 3.1318 - val_accuracy: 0.4967\n",
            "Epoch 964/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.9277 - val_loss: 3.1370 - val_accuracy: 0.4967\n",
            "Epoch 965/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.9242 - val_loss: 3.1512 - val_accuracy: 0.4933\n",
            "Epoch 966/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.9230 - val_loss: 3.1448 - val_accuracy: 0.5033\n",
            "Epoch 967/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3566 - accuracy: 0.9264 - val_loss: 3.1557 - val_accuracy: 0.4967\n",
            "Epoch 968/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.9426 - val_loss: 3.1592 - val_accuracy: 0.4933\n",
            "Epoch 969/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.9219 - val_loss: 3.1528 - val_accuracy: 0.5033\n",
            "Epoch 970/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.9262 - val_loss: 3.1615 - val_accuracy: 0.5000\n",
            "Epoch 971/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.9143 - val_loss: 3.1505 - val_accuracy: 0.5000\n",
            "Epoch 972/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.9124 - val_loss: 3.1546 - val_accuracy: 0.5033\n",
            "Epoch 973/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.9111 - val_loss: 3.1414 - val_accuracy: 0.4933\n",
            "Epoch 974/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.9260 - val_loss: 3.1785 - val_accuracy: 0.4967\n",
            "Epoch 975/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.9324 - val_loss: 3.1537 - val_accuracy: 0.5033\n",
            "Epoch 976/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3551 - accuracy: 0.9295 - val_loss: 3.1531 - val_accuracy: 0.4967\n",
            "Epoch 977/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.9194 - val_loss: 3.1544 - val_accuracy: 0.5000\n",
            "Epoch 978/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.9290 - val_loss: 3.1850 - val_accuracy: 0.4933\n",
            "Epoch 979/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.9268 - val_loss: 3.1797 - val_accuracy: 0.5033\n",
            "Epoch 980/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.9232 - val_loss: 3.1629 - val_accuracy: 0.4933\n",
            "Epoch 981/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.9174 - val_loss: 3.1750 - val_accuracy: 0.4967\n",
            "Epoch 982/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.9241 - val_loss: 3.1481 - val_accuracy: 0.5000\n",
            "Epoch 983/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.9232 - val_loss: 3.1730 - val_accuracy: 0.5033\n",
            "Epoch 984/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.9195 - val_loss: 3.1775 - val_accuracy: 0.4933\n",
            "Epoch 985/1000\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.9225 - val_loss: 3.1580 - val_accuracy: 0.4933\n",
            "Epoch 986/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.9380 - val_loss: 3.1509 - val_accuracy: 0.5000\n",
            "Epoch 987/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.9227 - val_loss: 3.1751 - val_accuracy: 0.4933\n",
            "Epoch 988/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.9261 - val_loss: 3.1860 - val_accuracy: 0.4933\n",
            "Epoch 989/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.9169 - val_loss: 3.1649 - val_accuracy: 0.4900\n",
            "Epoch 990/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.9325 - val_loss: 3.1876 - val_accuracy: 0.4967\n",
            "Epoch 991/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.9196 - val_loss: 3.1784 - val_accuracy: 0.4967\n",
            "Epoch 992/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.9280 - val_loss: 3.2154 - val_accuracy: 0.4933\n",
            "Epoch 993/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.9217 - val_loss: 3.2026 - val_accuracy: 0.4967\n",
            "Epoch 994/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.9310 - val_loss: 3.1894 - val_accuracy: 0.4967\n",
            "Epoch 995/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.9352 - val_loss: 3.1899 - val_accuracy: 0.4967\n",
            "Epoch 996/1000\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.4262 - accuracy: 0.9083 - val_loss: 3.1976 - val_accuracy: 0.4933\n",
            "Epoch 997/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.9251 - val_loss: 3.1895 - val_accuracy: 0.4933\n",
            "Epoch 998/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.9281 - val_loss: 3.1744 - val_accuracy: 0.4900\n",
            "Epoch 999/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.9352 - val_loss: 3.2047 - val_accuracy: 0.5000\n",
            "Epoch 1000/1000\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.9449 - val_loss: 3.1878 - val_accuracy: 0.4933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "AMexnoMMYx_r",
        "outputId": "72f2231f-631c-421d-ff3d-05f92e6b2a12"
      },
      "source": [
        "'''\r\n",
        "배치 사이즈와 에포크\r\n",
        "\r\n",
        "케라스에서 만든 모델을 학습할 때는 fit() 함수를 사용합니다\r\n",
        "\r\n",
        "model.fit(x, y, batch_size = 32, epochs = 10)\r\n",
        "\r\n",
        "x : 입럭 데이터 / y : 라벨 값 / batch_size : 몇 개의 샘플로 가중치를 갱신할 것인지 지정 / epochs : 학습 반복 횟수\r\n",
        "'''\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n배치 사이즈와 에포크\\n\\n케라스에서 만든 모델을 학습할 때는 fit() 함수를 사용합니다\\n\\nmodel.fit(x, y, batch_size = 32, epochs = 10)\\n\\nx : 입럭 데이터 / y : 라벨 값 / batch_size : 몇 개의 샘플로 가중치를 갱신할 것인지 지정 / epochs : 학습 반복 횟수\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7uEn5yUfem_"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}